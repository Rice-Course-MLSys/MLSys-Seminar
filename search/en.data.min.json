[{"id":0,"href":"/MLSys-Seminar/collapse/level-1/","title":"Level 1","parent":"Collapse","content":"Level 1\nLevel 1.1 Level 1.2 ","description":"Level 1\nLevel 1.1 Level 1.2 "},{"id":1,"href":"/MLSys-Seminar/collapse/level-2/","title":"Level 2","parent":"Collapse","content":"Level-2\n","description":"Level-2\n"},{"id":2,"href":"/MLSys-Seminar/staffers/fanjiang/","title":"Fanjiang","parent":"Staff","content":"","description":""},{"id":3,"href":"/MLSys-Seminar/staffers/yuke/","title":"Yuke","parent":"Staff","content":"","description":""},{"id":4,"href":"/MLSys-Seminar/toc-tree/level-1/","title":"Level 1","parent":"ToC-Tree","content":"Level 1\nLevel 1.1 Level 1.2 Level 1.3 Level 1.3.1 ","description":"Level 1\nLevel 1.1 Level 1.2 Level 1.3 Level 1.3.1 "},{"id":5,"href":"/MLSys-Seminar/toc-tree/level-2/","title":"Level 2","parent":"ToC-Tree","content":"Level-2\n","description":"Level-2\n"},{"id":6,"href":"/MLSys-Seminar/about/","title":"About","parent":"Machine Learning System Seminar","content":"This course is a research seminar focused on efficient machine learning systems. This is a crucial area as modern deep neural networks, especially Large Language Models (LLMs) and generative models, demand extraordinary levels of computation. This computational cost hinders their deployment and scaling, creating significant challenges for both resource-constrained edge devices and large-scale cloud infrastructure. This course delves into the cutting-edge techniques that enable powerful and efficient AI applications.\nWe will cover the full stack of AI systems, from low-level kernel optimizations to high-level algorithm and model designs. The course will survey and dissect recent influential research papers in the field.\nTopics include:\nKernel-Level Optimizations: I/O-aware and exact attention mechanisms (e.g., FlashAttention), sparse attention, and custom kernel generation with AI compilers (e.g., TVM, MLIR).\nEfficient LLMs: State-of-the-art techniques for efficient training (e.g., ZeRO, LoRA), inference (e.g., vLLM, speculative decoding), model compression (quantization, pruning), and long-context optimizations.\nEfficient Model Architectures: Design principles for efficient models, including Mixture-of-Experts (MoE) and State Space Models (Mamba).\nGenerative AI Systems: Optimization techniques for emerging modalities, including efficient video generation, super-resolution, and understanding.\nSecure and Private AI: Methods for ensuring model and data security, including watermarking and encryption in the context of large models.models\nThis course is run as a research seminar. The focus will be on critically reading, presenting, and discussing influential papers in efficient AI systems. Through weekly readings and student-led presentations, participants will gain a deep understanding of the key challenges, foundational techniques, and future directions in the field. Approximately every two weeks, we will also host a guest lecture from a leading researcher or engineer from academia or industry, providing direct insights into the state-of-the-art.\nLecture Videos: [Link to be provided]\nTime: Friday 3:00 PM–4:15 PM CST\nLocation: [TBA]\nOffice Hour: [TBA]\nDiscussion: [TBA]\nHomework Submission: [TBA]\nContact:\n[TBD]\nPrerequisites:\n[TBD]\n","description":"This course is a research seminar focused on efficient machine learning systems. This is a crucial area as modern deep neural networks, especially Large Language Models (LLMs) and generative models, demand extraordinary levels of computation. This computational cost hinders their deployment and scaling, creating significant challenges for both resource-constrained edge devices and large-scale cloud infrastructure. This course delves into the cutting-edge techniques that enable powerful and efficient AI applications.\nWe will cover the full stack of AI systems, from low-level kernel optimizations to high-level algorithm and model designs. The course will survey and dissect recent influential research papers in the field.\n"},{"id":7,"href":"/MLSys-Seminar/collapse/","title":"Collapse","parent":"Machine Learning System Seminar","content":"Demo collapsible menu entries.\n","description":"Demo collapsible menu entries.\n"},{"id":8,"href":"/MLSys-Seminar/collapse/level-1/level-1-1/","title":"Level 1.1","parent":"Level 1","content":"Level 1.1\n","description":"Level 1.1\n"},{"id":9,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-1/","title":"Level 1.1","parent":"Level 1","content":"Level 1.1\n","description":"Level 1.1\n"},{"id":10,"href":"/MLSys-Seminar/collapse/level-1/level-1-2/","title":"Level 1.2","parent":"Level 1","content":"Level 1.2\n","description":"Level 1.2\n"},{"id":11,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-2/","title":"Level 1.2","parent":"Level 1","content":"Level 1.2\n","description":"Level 1.2\n"},{"id":12,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-3/","title":"Level 1.3","parent":"Level 1","content":"Level 1.3\nLevel 1.3.1 ","description":"Level 1.3\nLevel 1.3.1 "},{"id":13,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-3/level-1-3-1/","title":"Level 1.3.1","parent":"Level 1.3","content":"Level 1.3.1\n","description":"Level 1.3.1\n"},{"id":14,"href":"/MLSys-Seminar/collapse/level-2/level-2-1/","title":"Level 2.1","parent":"Level 2","content":"Level 2.1\n","description":"Level 2.1\n"},{"id":15,"href":"/MLSys-Seminar/toc-tree/level-2/level-2-1/","title":"Level 2.1","parent":"Level 2","content":"Level 2.1\n","description":"Level 2.1\n"},{"id":16,"href":"/MLSys-Seminar/collapse/level-2/level-2-2/","title":"Level 2.2","parent":"Level 2","content":"Level 2.2\n","description":"Level 2.2\n"},{"id":17,"href":"/MLSys-Seminar/toc-tree/level-2/level-2-2/","title":"Level 2.2","parent":"Level 2","content":"Level 2.2\n","description":"Level 2.2\n"},{"id":18,"href":"/MLSys-Seminar/logistics/","title":"Logistics","parent":"Machine Learning System Seminar","content":" Grading Under Discussion\n","description":" Grading Under Discussion\n"},{"id":19,"href":"/MLSys-Seminar/","title":"Machine Learning System Seminar","parent":"","content":" COMP 620 - Fall 2025 This course is a research seminar focused on efficient machine learning systems. This is a crucial area as modern deep neural networks, especially Large Language Models (LLMs) and generative models, demand extraordinary levels of computation. This computational cost hinders their deployment and scaling, creating significant challenges for both resource-constrained edge devices and large-scale cloud infrastructure. This course delves into the cutting-edge techniques that enable powerful and efficient AI applications.\nWe will cover the full stack of AI systems, from low-level kernel optimizations to high-level algorithm and model designs. The course will survey and dissect recent influential research papers in the field.\nTopics include:\nKernel-Level Optimizations: I/O-aware and exact attention mechanisms (e.g., FlashAttention), sparse attention, and custom kernel generation with AI compilers (e.g., TVM, MLIR).\nEfficient LLMs: State-of-the-art techniques for efficient training (e.g., ZeRO, LoRA), inference (e.g., vLLM, speculative decoding), model compression (quantization, pruning), and long-context optimizations.\nEfficient Model Architectures: Design principles for efficient models, including Mixture-of-Experts (MoE) and State Space Models (Mamba).\nGenerative AI Systems: Optimization techniques for emerging modalities, including efficient video generation, super-resolution, and understanding.\nSecure and Private AI: Methods for ensuring model and data security, including watermarking and encryption in the context of large models.models\nThis course is run as a research seminar. The focus will be on critically reading, presenting, and discussing influential papers in efficient AI systems. Through weekly readings and student-led presentations, participants will gain a deep understanding of the key challenges, foundational techniques, and future directions in the field. Approximately every two weeks, we will also host a guest lecture from a leading researcher or engineer from academia or industry, providing direct insights into the state-of-the-art.\nLecture Videos: [Link to be provided]\nTime: Friday 3:00 PM–4:15 PM CST\nLocation: [TBA]\nOffice Hour: [TBA]\nDiscussion: [TBA]\nHomework Submission: [TBA]\nContact:\n[TBD]\nPrerequisites:\n[TBD]\n","description":" COMP 620 - Fall 2025 This course is a research seminar focused on efficient machine learning systems. This is a crucial area as modern deep neural networks, especially Large Language Models (LLMs) and generative models, demand extraordinary levels of computation. This computational cost hinders their deployment and scaling, creating significant challenges for both resource-constrained edge devices and large-scale cloud infrastructure. This course delves into the cutting-edge techniques that enable powerful and efficient AI applications.\n"},{"id":20,"href":"/MLSys-Seminar/materials/","title":"Materials","parent":"Machine Learning System Seminar","content":" Chapter I: Kernel Related Subtopic 1: I/O Aware \u0026amp; Exact Attention Paper Link FlashAttention 1, 2, 3 PDF, PDF, PDF PagedAttention (vLLM) PDF SGLang PDF FlexAttention PDF FlashInfer PDF SpargeAttention PDF SageAttention 1,2 PDF, PDF Subtopic 2: Sparse Attention Paper Link DejaVu PDF H2O PDF SpAttn PDF MoE PDF Deepseek-MoE PDF Subtopic 3: Kernel Generation \u0026amp; Compiler Paper Link TVM PDF Ansor PDF MLIR PDF Subtopic 4: Execution Optimization/Serving Paper Link Alpa PDF Orca PDF FlexGen PDF ZeRO-Offloading PDF Megatron-LM PDF FlashDecoding++ PDF SarathiServe PDF Chapter II: Efficient LLM Subtopic 1: LLM 101 Paper Link Attention is All You Need PDF BERT PDF GPT-3 PDF Scaling Laws PDF RLHF PDF PPO/DPO PDF , PDF Subtopic 2: Efficient Inference \u0026amp; Long-context Paper Link Streaming LLM \u0026amp; DuoAttention PDF, PDF MInference PDF H2O PDF TOVA/KIVI PDF, PDF Speculative Decoding PDF, PDF Multi-token prediction: Deepseek-v3 PDF Subtopic 3: Model Compression (Quant \u0026amp; Pruning) Paper Link LLM.int8()/GPTQ PDF, PDF AWQ PDF LLM Pruner PDF ShearedLlama PDF Subtopic 4: Efficient Training Paper Link ZeRO PDF Megatron-LM PDF LoRA \u0026amp; QLoRA PDF, PDF Subtopic 5: Efficient Model Designs Paper Link Swtich Transformers/Outrageously Large Neural Networks PDF, PDF MLA Attention PDF Mamba PDF Chapter III: Video Generation Subtopic 1: SOTA/Baseline Model Paper Link CogVideoX PDF HunyuanVideo PDF WAN PDF Seaweed-7B PDF Subtopic 2: Optimization Techniques Paper Link Pruning UniCP Cache PDF, need to add more.. Compression PDF Sparsity PDF Subtopic 3: Long Video Generation Paper Link Tuning-Free Multi-Event Long Video Generation PDF Long Context Tuning for Video Generation PDF One-Minute Video Generation with Test-Time Training PDF SKYREELS-V2: INFINITE-LENGTH FILM GENERATIVE MODEL PDF Subtopic 4: Video Super Resolution Paper Link SeedVR PDF MGLD-VSR PDF DynamicScaler PDF Chapter IV: Secure LLM Subtopic 1: Diffusion Model/Flow Matching Subtopic 2: Watermarking Subtopic 3: Efficient CNN Subtopic 4: Encryption Chapter V: MLLM Video Understanding Subtopic 1: SOTA/Baseline Subtopic 2: Optimization Techniques Subtopic 3: Algorithm Design ","description":" Chapter I: Kernel Related Subtopic 1: I/O Aware \u0026amp; Exact Attention Paper Link FlashAttention 1, 2, 3 PDF, PDF, PDF PagedAttention (vLLM) PDF SGLang PDF FlexAttention PDF FlashInfer PDF SpargeAttention PDF SageAttention 1,2 PDF, PDF Subtopic 2: Sparse Attention Paper Link DejaVu PDF H2O PDF SpAttn PDF MoE PDF Deepseek-MoE PDF Subtopic 3: Kernel Generation \u0026amp; Compiler Paper Link TVM PDF Ansor PDF MLIR PDF Subtopic 4: Execution Optimization/Serving Paper Link Alpa PDF Orca PDF FlexGen PDF ZeRO-Offloading PDF Megatron-LM PDF FlashDecoding++ PDF SarathiServe PDF Chapter II: Efficient LLM Subtopic 1: LLM 101 Paper Link Attention is All You Need PDF BERT PDF GPT-3 PDF Scaling Laws PDF RLHF PDF PPO/DPO PDF , PDF Subtopic 2: Efficient Inference \u0026amp; Long-context Paper Link Streaming LLM \u0026amp; DuoAttention PDF, PDF MInference PDF H2O PDF TOVA/KIVI PDF, PDF Speculative Decoding PDF, PDF Multi-token prediction: Deepseek-v3 PDF Subtopic 3: Model Compression (Quant \u0026amp; Pruning) Paper Link LLM.int8()/GPTQ PDF, PDF AWQ PDF LLM Pruner PDF ShearedLlama PDF Subtopic 4: Efficient Training Paper Link ZeRO PDF Megatron-LM PDF LoRA \u0026amp; QLoRA PDF, PDF Subtopic 5: Efficient Model Designs Paper Link Swtich Transformers/Outrageously Large Neural Networks PDF, PDF MLA Attention PDF Mamba PDF Chapter III: Video Generation Subtopic 1: SOTA/Baseline Model Paper Link CogVideoX PDF HunyuanVideo PDF WAN PDF Seaweed-7B PDF Subtopic 2: Optimization Techniques Paper Link Pruning UniCP Cache PDF, need to add more.. Compression PDF Sparsity PDF Subtopic 3: Long Video Generation Paper Link Tuning-Free Multi-Event Long Video Generation PDF Long Context Tuning for Video Generation PDF One-Minute Video Generation with Test-Time Training PDF SKYREELS-V2: INFINITE-LENGTH FILM GENERATIVE MODEL PDF Subtopic 4: Video Super Resolution Paper Link SeedVR PDF MGLD-VSR PDF DynamicScaler PDF Chapter IV: Secure LLM Subtopic 1: Diffusion Model/Flow Matching Subtopic 2: Watermarking Subtopic 3: Efficient CNN Subtopic 4: Encryption Chapter V: MLLM Video Understanding Subtopic 1: SOTA/Baseline Subtopic 2: Optimization Techniques Subtopic 3: Algorithm Design "},{"id":21,"href":"/MLSys-Seminar/schedule/","title":"Schedule","parent":"Machine Learning System Seminar","content":" Date Lecture Materials Logistics Sep 5 (3:00 PM–4:15 PM) Lecture 1: Introduction [Slides] Chapter I: Kernel Related Sep 12 (3:00 PM–4:15 PM) Lecture 2: I/O Aware \u0026amp; Exact Attention [Slides] [Link] Sep 19 (3:00 PM–4:15 PM) Lecture 3: Sparse Attention [Slides] [Link] Sep 26 (3:00 PM–4:15 PM) Lecture 4: Kernel Generation \u0026amp; Compiler [Slides] [Link] Oct 3 (3:00 PM–4:15 PM) Lecture 5: Execution Optimization/Serving [Slides] [Link] Chapter II: Efficient LLM Oct 10 (3:00 PM–4:15 PM) Lecture 6: LLM 101 [Slides] [Link] Oct 17 (3:00 PM–4:15 PM) Lecture 7: Efficient Inference \u0026amp; Long-context [Slides] [Link] Oct 24 (3:00 PM–4:15 PM) Lecture 8: Model Compression (Quant \u0026amp; Pruning) [Slides] [Link] Oct 31 (3:00 PM–4:15 PM) Lecture 9: Efficient Training [Slides] [Link] Nov 7 (3:00 PM–4:15 PM) Lecture 10: Efficient Model Designs [Slides] [Link] Chapter III: Video Generation Nov 14 (3:00 PM–4:15 PM) Lecture 11: SOTA/Baseline Model [Slides] [Link] Nov 21 (3:00 PM–4:15 PM) Lecture 12: Optimization Techniques [Slides] [Link] Nov 28 (3:00 PM–4:15 PM) Lecture 13: Long Video Generation [Slides] [Link] Dec 5 (3:00 PM–4:15 PM) Lecture 14: Video Super Resolution [Slides] [Link] Chapter IV: Secure LLM Dec 12 (3:00 PM–4:15 PM) Lecture 15: Diffusion Model/Flow Matching [Slides] [Link] Dec 19 (3:00 PM–4:15 PM) Lecture 16: Watermarking [Slides] [Link] Dec 26 (3:00 PM–4:15 PM) Lecture 17: Efficient CNN [Slides] [Link] Jan 2 (3:00 PM–4:15 PM) Lecture 18: Encryption [Slides] [Link] Chapter V: MLLM Video Understanding Jan 9 (3:00 PM–4:15 PM) Lecture 19: SOTA/Baseline [Slides] [Link] Jan 16 (3:00 PM–4:15 PM) Lecture 20: Optimization Techniques [Slides] [Link] Jan 23 (3:00 PM–4:15 PM) Lecture 21: Algorithm Design [Slides] [Link] ","description":" Date Lecture Materials Logistics Sep 5 (3:00 PM–4:15 PM) Lecture 1: Introduction [Slides] Chapter I: Kernel Related Sep 12 (3:00 PM–4:15 PM) Lecture 2: I/O Aware \u0026amp; Exact Attention [Slides] [Link] Sep 19 (3:00 PM–4:15 PM) Lecture 3: Sparse Attention [Slides] [Link] Sep 26 (3:00 PM–4:15 PM) Lecture 4: Kernel Generation \u0026amp; Compiler [Slides] [Link] Oct 3 (3:00 PM–4:15 PM) Lecture 5: Execution Optimization/Serving [Slides] [Link] Chapter II: Efficient LLM Oct 10 (3:00 PM–4:15 PM) Lecture 6: LLM 101 [Slides] [Link] Oct 17 (3:00 PM–4:15 PM) Lecture 7: Efficient Inference \u0026amp; Long-context [Slides] [Link] Oct 24 (3:00 PM–4:15 PM) Lecture 8: Model Compression (Quant \u0026amp; Pruning) [Slides] [Link] Oct 31 (3:00 PM–4:15 PM) Lecture 9: Efficient Training [Slides] [Link] Nov 7 (3:00 PM–4:15 PM) Lecture 10: Efficient Model Designs [Slides] [Link] Chapter III: Video Generation Nov 14 (3:00 PM–4:15 PM) Lecture 11: SOTA/Baseline Model [Slides] [Link] Nov 21 (3:00 PM–4:15 PM) Lecture 12: Optimization Techniques [Slides] [Link] Nov 28 (3:00 PM–4:15 PM) Lecture 13: Long Video Generation [Slides] [Link] Dec 5 (3:00 PM–4:15 PM) Lecture 14: Video Super Resolution [Slides] [Link] Chapter IV: Secure LLM Dec 12 (3:00 PM–4:15 PM) Lecture 15: Diffusion Model/Flow Matching [Slides] [Link] Dec 19 (3:00 PM–4:15 PM) Lecture 16: Watermarking [Slides] [Link] Dec 26 (3:00 PM–4:15 PM) Lecture 17: Efficient CNN [Slides] [Link] Jan 2 (3:00 PM–4:15 PM) Lecture 18: Encryption [Slides] [Link] Chapter V: MLLM Video Understanding Jan 9 (3:00 PM–4:15 PM) Lecture 19: SOTA/Baseline [Slides] [Link] Jan 16 (3:00 PM–4:15 PM) Lecture 20: Optimization Techniques [Slides] [Link] Jan 23 (3:00 PM–4:15 PM) Lecture 21: Algorithm Design [Slides] [Link] "},{"id":22,"href":"/MLSys-Seminar/staffers/","title":"Staff","parent":"Machine Learning System Seminar","content":" ","description":""},{"id":23,"href":"/MLSys-Seminar/tags/","title":"Tags","parent":"Machine Learning System Seminar","content":"","description":""},{"id":24,"href":"/MLSys-Seminar/toc-tree/","title":"ToC-Tree","parent":"Machine Learning System Seminar","content":"This is just a demo section for the toc-tree shortcode.\nLevel 1 Level 1.1 Level 1.2 Level 1.3 Level 1.3.1 Level 2 Level 2.1 Level 2.2 ","description":"This is just a demo section for the toc-tree shortcode.\nLevel 1 Level 1.1 Level 1.2 Level 1.3 Level 1.3.1 Level 2 Level 2.1 Level 2.2 "}]