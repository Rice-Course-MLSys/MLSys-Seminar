[{"id":0,"href":"/MLSys-Seminar/collapse/level-1/","title":"Level 1","parent":"Collapse","content":"Level 1\nLevel 1.1 Level 1.2 ","description":"Level 1\nLevel 1.1 Level 1.2 "},{"id":1,"href":"/MLSys-Seminar/collapse/level-2/","title":"Level 2","parent":"Collapse","content":"Level-2\n","description":"Level-2\n"},{"id":2,"href":"/MLSys-Seminar/staffers/yuke/","title":"Yuke","parent":"Staff","content":"","description":""},{"id":3,"href":"/MLSys-Seminar/toc-tree/level-1/","title":"Level 1","parent":"ToC-Tree","content":"Level 1\nLevel 1.1 Level 1.2 Level 1.3 Level 1.3.1 ","description":"Level 1\nLevel 1.1 Level 1.2 Level 1.3 Level 1.3.1 "},{"id":4,"href":"/MLSys-Seminar/toc-tree/level-2/","title":"Level 2","parent":"ToC-Tree","content":"Level-2\n","description":"Level-2\n"},{"id":5,"href":"/MLSys-Seminar/about/","title":"About","parent":"Machine Learning System Seminar","content":"This course is a research seminar focused on efficient machine learning systems. This is a crucial area as modern deep neural networks, especially Large Language Models (LLMs) and generative models, demand extraordinary levels of computation. This computational cost hinders their deployment and scaling, creating significant challenges for both resource-constrained edge devices and large-scale cloud infrastructure. This course delves into the cutting-edge techniques that enable powerful and efficient AI applications.\nWe will cover the full stack of AI systems, from low-level kernel optimizations to high-level algorithm and model designs. The course will survey and dissect recent influential research papers in the field.\nTopics include:\nKernel-Level Optimizations: I/O-aware and exact attention mechanisms (e.g., FlashAttention), sparse attention, and custom kernel generation with AI compilers (e.g., TVM, MLIR).\nEfficient LLMs: State-of-the-art techniques for efficient training (e.g., ZeRO, LoRA), inference (e.g., vLLM, speculative decoding), model compression (quantization, pruning), and long-context optimizations.\nEfficient Model Architectures: Design principles for efficient models, including Mixture-of-Experts (MoE) and State Space Models (Mamba).\nGenerative AI Systems: Optimization techniques for emerging modalities, including efficient video generation, super-resolution, and understanding.\nSecure and Private AI: Methods for ensuring model and data security, including watermarking and encryption in the context of large models.models\nThis course is run as a research seminar. The focus will be on critically reading, presenting, and discussing influential papers in efficient AI systems. Through weekly readings and student-led presentations, participants will gain a deep understanding of the key challenges, foundational techniques, and future directions in the field. Approximately every two weeks, we will also host a guest lecture from a leading researcher or engineer from academia or industry, providing direct insights into the state-of-the-art.\nLecture Videos: [Link to be provided]\nTime: Friday 3:00 PM–4:15 PM CST\nLocation: [TBA]\nOffice Hour: [TBA]\nDiscussion: [TBA]\nHomework Submission: [TBA]\nContact:\n[TBD]\nPrerequisites:\n[TBD]\n","description":"This course is a research seminar focused on efficient machine learning systems. This is a crucial area as modern deep neural networks, especially Large Language Models (LLMs) and generative models, demand extraordinary levels of computation. This computational cost hinders their deployment and scaling, creating significant challenges for both resource-constrained edge devices and large-scale cloud infrastructure. This course delves into the cutting-edge techniques that enable powerful and efficient AI applications.\nWe will cover the full stack of AI systems, from low-level kernel optimizations to high-level algorithm and model designs. The course will survey and dissect recent influential research papers in the field.\n"},{"id":6,"href":"/MLSys-Seminar/collapse/","title":"Collapse","parent":"Machine Learning System Seminar","content":"Demo collapsible menu entries.\n","description":"Demo collapsible menu entries.\n"},{"id":7,"href":"/MLSys-Seminar/schedule/","title":"Guest Lectures","parent":"Machine Learning System Seminar","content":" Date Lecture Materials Sep 53:00 PM–3:40 PM Guest Lecture: Boyuan Feng [Slides] BioBoyuan Feng is a PyTorch Core Developer working on PyTorch Compiler, Inductor, CUDAGraph, and Flex Attention.Lecture AbstractFlexAttention is a novel compiler-driven programming model that allows implementing the majority of attention variants in a few lines of idiomatic PyTorch code. Since its release in PyTorch 2.5.0, many ML researchers have utilized it to customize their attention kernels without writing kernel code. In this talk, we present recent advances in FlexAttention. More details on our MLSys'25 paper (https://arxiv.org/pdf/2412.05496) and PyTorch Blog (https://pytorch.org/blog/flexattention-for-inference/)! Sep 193:00 PM–3:40 PM Guest Lecture: Yue Guan [Slides] BioYue Guan is a postdoctoral researcher at the University of California, San Diego, working with Prof. Yufei Ding in the Picasso Lab. He received his Ph.D. in Computer Science from Shanghai Jiao Tong University under the supervision of Prof. Jingwen Leng. His research focuses on efficient deep learning systems, spanning model compression, compiler optimization, and system design. His work has been published in top venues such as SOSP, OSDI, ASPLOS and HPCA.Lecture AbstractThe rapid growth of large language models (LLMs) requires better compilers for efficient use of multi-GPU systems. In this talk, I will introduce Mercury, a compiler that manages remote GPU memory as part of the memory hierarchy to optimize computation, storage, and communication. I will also present KPerfIR, a tool that adds profiling directly into the compilation process to help analyze GPU kernel performance. These approaches show how integrating optimization and performance analysis in compilers can improve the scalability and efficiency of LLMs. Oct 33:00 PM–3:40 PM Guest Lecture: Liangyu Zhao [Slides] BioLiangyu Zhao is a fourth-year PhD student at the University of Washington, advised by Prof. Arvind Krishnamurthy. His research focuses on machine learning systems, with an emphasis on network communication for distributed machine learning. Currently, he is a research scientist intern at Meta AI \u0026amp; Systems Co-Design team.Lecture AbstractAs modern DNN models grow ever larger, collective communications between the accelerators (allreduce, etc.) emerge as a significant performance bottleneck. Designing efficient communication schedules is challenging, given today\u0026rsquo;s heterogeneous and diverse network fabrics. We present ForestColl, a tool that generates throughput-optimal schedules for any network topology. ForestColl constructs broadcast/aggregation spanning trees as the communication schedule, achieving theoretical optimality. Its schedule generation runs in strongly polynomial time and is highly scalable. ForestColl supports any network fabrics, including both switching fabrics and direct accelerator connections. We evaluated ForestColl on multi-box AMD MI250 and NVIDIA DGX A100 platforms. ForestColl showed significant improvements over the vendors\u0026rsquo; own optimized communication libraries, RCCL and NCCL, across various settings and in LLM training. ForestColl also outperformed other state-of-the-art schedule generation techniques with both more efficient generated schedules and substantially faster schedule generation speed. Oct 313:00 PM–3:40 PM Guest Lecture: Zhuang Wang [Slides] BioZhuang Wang is an Applied Scientist at Amazon Web Services AI. He received his Ph.D. degree in Computer Science from Rice University in 2023, fortunately advised by Prof. T. S. Eugene Ng. His current research interests focus on efficient training and inference systems for large language models.Lecture AbstractFrequent failures are observed during large model training due to large-scale resources involved and extended training time. This talk presents Gemini, a distributed training system that enables fast failure recovery for large model training by checkpointing to CPU memory of the host machines with much larger aggregated bandwidth. However, two challenges prevent naïvely checkpointing to CPU memory. First, the availability of checkpoints in CPU memory cannot be guaranteed when failures occur. Second, since the communication traffic for training and checkpointing share the same network, checkpoint traffic can interfere with training traffic and harm training throughput. To address these two challenges, we propose: 1) a provably near-optimal checkpoint placement strategy to maximize the probability of failure recovery from checkpoints in CPU memory; and 2) a checkpoint traffic scheduling algorithm to minimize, if not eliminate, the interference of checkpoint traffic on model training. Our evaluation shows that Gemini achieves optimal checkpoint frequency, i.e., every iteration, and incurs no overhead on training throughput for large model training. Nov 143:00 PM–3:40 PM Guest Lecture: Yixin Dong [Slides] BioTo Be Finished Lecture AbstractTopic Not Confirmed ","description":" Date Lecture Materials Sep 53:00 PM–3:40 PM Guest Lecture: Boyuan Feng [Slides] BioBoyuan Feng is a PyTorch Core Developer working on PyTorch Compiler, Inductor, CUDAGraph, and Flex Attention.Lecture AbstractFlexAttention is a novel compiler-driven programming model that allows implementing the majority of attention variants in a few lines of idiomatic PyTorch code. Since its release in PyTorch 2.5.0, many ML researchers have utilized it to customize their attention kernels without writing kernel code. In this talk, we present recent advances in FlexAttention. More details on our MLSys'25 paper (https://arxiv.org/pdf/2412.05496) and PyTorch Blog (https://pytorch.org/blog/flexattention-for-inference/)! Sep 193:00 PM–3:40 PM Guest Lecture: Yue Guan [Slides] BioYue Guan is a postdoctoral researcher at the University of California, San Diego, working with Prof. Yufei Ding in the Picasso Lab. He received his Ph.D. in Computer Science from Shanghai Jiao Tong University under the supervision of Prof. Jingwen Leng. His research focuses on efficient deep learning systems, spanning model compression, compiler optimization, and system design. His work has been published in top venues such as SOSP, OSDI, ASPLOS and HPCA.Lecture AbstractThe rapid growth of large language models (LLMs) requires better compilers for efficient use of multi-GPU systems. In this talk, I will introduce Mercury, a compiler that manages remote GPU memory as part of the memory hierarchy to optimize computation, storage, and communication. I will also present KPerfIR, a tool that adds profiling directly into the compilation process to help analyze GPU kernel performance. These approaches show how integrating optimization and performance analysis in compilers can improve the scalability and efficiency of LLMs. Oct 33:00 PM–3:40 PM Guest Lecture: Liangyu Zhao [Slides] BioLiangyu Zhao is a fourth-year PhD student at the University of Washington, advised by Prof. Arvind Krishnamurthy. His research focuses on machine learning systems, with an emphasis on network communication for distributed machine learning. Currently, he is a research scientist intern at Meta AI \u0026amp; Systems Co-Design team.Lecture AbstractAs modern DNN models grow ever larger, collective communications between the accelerators (allreduce, etc.) emerge as a significant performance bottleneck. Designing efficient communication schedules is challenging, given today\u0026rsquo;s heterogeneous and diverse network fabrics. We present ForestColl, a tool that generates throughput-optimal schedules for any network topology. ForestColl constructs broadcast/aggregation spanning trees as the communication schedule, achieving theoretical optimality. Its schedule generation runs in strongly polynomial time and is highly scalable. ForestColl supports any network fabrics, including both switching fabrics and direct accelerator connections. We evaluated ForestColl on multi-box AMD MI250 and NVIDIA DGX A100 platforms. ForestColl showed significant improvements over the vendors\u0026rsquo; own optimized communication libraries, RCCL and NCCL, across various settings and in LLM training. ForestColl also outperformed other state-of-the-art schedule generation techniques with both more efficient generated schedules and substantially faster schedule generation speed. Oct 313:00 PM–3:40 PM Guest Lecture: Zhuang Wang [Slides] BioZhuang Wang is an Applied Scientist at Amazon Web Services AI. He received his Ph.D. degree in Computer Science from Rice University in 2023, fortunately advised by Prof. T. S. Eugene Ng. His current research interests focus on efficient training and inference systems for large language models.Lecture AbstractFrequent failures are observed during large model training due to large-scale resources involved and extended training time. This talk presents Gemini, a distributed training system that enables fast failure recovery for large model training by checkpointing to CPU memory of the host machines with much larger aggregated bandwidth. However, two challenges prevent naïvely checkpointing to CPU memory. First, the availability of checkpoints in CPU memory cannot be guaranteed when failures occur. Second, since the communication traffic for training and checkpointing share the same network, checkpoint traffic can interfere with training traffic and harm training throughput. To address these two challenges, we propose: 1) a provably near-optimal checkpoint placement strategy to maximize the probability of failure recovery from checkpoints in CPU memory; and 2) a checkpoint traffic scheduling algorithm to minimize, if not eliminate, the interference of checkpoint traffic on model training. Our evaluation shows that Gemini achieves optimal checkpoint frequency, i.e., every iteration, and incurs no overhead on training throughput for large model training. Nov 143:00 PM–3:40 PM Guest Lecture: Yixin Dong [Slides] BioTo Be Finished Lecture AbstractTopic Not Confirmed "},{"id":8,"href":"/MLSys-Seminar/collapse/level-1/level-1-1/","title":"Level 1.1","parent":"Level 1","content":"Level 1.1\n","description":"Level 1.1\n"},{"id":9,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-1/","title":"Level 1.1","parent":"Level 1","content":"Level 1.1\n","description":"Level 1.1\n"},{"id":10,"href":"/MLSys-Seminar/collapse/level-1/level-1-2/","title":"Level 1.2","parent":"Level 1","content":"Level 1.2\n","description":"Level 1.2\n"},{"id":11,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-2/","title":"Level 1.2","parent":"Level 1","content":"Level 1.2\n","description":"Level 1.2\n"},{"id":12,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-3/","title":"Level 1.3","parent":"Level 1","content":"Level 1.3\nLevel 1.3.1 ","description":"Level 1.3\nLevel 1.3.1 "},{"id":13,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-3/level-1-3-1/","title":"Level 1.3.1","parent":"Level 1.3","content":"Level 1.3.1\n","description":"Level 1.3.1\n"},{"id":14,"href":"/MLSys-Seminar/collapse/level-2/level-2-1/","title":"Level 2.1","parent":"Level 2","content":"Level 2.1\n","description":"Level 2.1\n"},{"id":15,"href":"/MLSys-Seminar/toc-tree/level-2/level-2-1/","title":"Level 2.1","parent":"Level 2","content":"Level 2.1\n","description":"Level 2.1\n"},{"id":16,"href":"/MLSys-Seminar/collapse/level-2/level-2-2/","title":"Level 2.2","parent":"Level 2","content":"Level 2.2\n","description":"Level 2.2\n"},{"id":17,"href":"/MLSys-Seminar/toc-tree/level-2/level-2-2/","title":"Level 2.2","parent":"Level 2","content":"Level 2.2\n","description":"Level 2.2\n"},{"id":18,"href":"/MLSys-Seminar/logistics/","title":"Logistics","parent":"Machine Learning System Seminar","content":" Grading The tentative grading breakdown for this course is as follows:\nParticipation: 15% Paper Presentation \u0026amp; Discussion: 15% Paper Summary: 10% Project Proposal: 5% Project Mid-Semester Presentations: 10% Project Final Presentations: 10% Project Final Report: 35% Groups All course activities (except your own participation) will be performed in groups of 4–5 students. Please form your group and submit your group membership and paper preferences by January 31. After this deadline, the instructor will assign remaining students into groups. Academic Integrity The University’s Honor Code applies to all activities in this course. All submitted materials (reading responses, project reports, presentation slides, etc.) must be your own work. If you reference or use external materials, you must cite them properly. AI Tool Policy Permitted: AI tools may be used for grammar checking and refining initial brainstorms. Not Permitted: The final written content, analyses, and code must be authored by the student. Students are fully responsible for the content they submit and must adhere to the Academic Integrity Policy. ","description":" Grading The tentative grading breakdown for this course is as follows:\nParticipation: 15% Paper Presentation \u0026amp; Discussion: 15% Paper Summary: 10% Project Proposal: 5% Project Mid-Semester Presentations: 10% Project Final Presentations: 10% Project Final Report: 35% Groups All course activities (except your own participation) will be performed in groups of 4–5 students. Please form your group and submit your group membership and paper preferences by January 31. After this deadline, the instructor will assign remaining students into groups. Academic Integrity The University’s Honor Code applies to all activities in this course. All submitted materials (reading responses, project reports, presentation slides, etc.) must be your own work. If you reference or use external materials, you must cite them properly. AI Tool Policy Permitted: AI tools may be used for grammar checking and refining initial brainstorms. Not Permitted: The final written content, analyses, and code must be authored by the student. Students are fully responsible for the content they submit and must adhere to the Academic Integrity Policy. "},{"id":19,"href":"/MLSys-Seminar/","title":"Machine Learning System Seminar","parent":"","content":" COMP 620 - Fall 2025 This course is a research seminar focused on efficient machine learning systems. This is a crucial area as modern deep neural networks, especially Large Language Models (LLMs) and generative models, demand extraordinary levels of computation. This computational cost hinders their deployment and scaling, creating significant challenges for both resource-constrained edge devices and large-scale cloud infrastructure. This course delves into the cutting-edge techniques that enable powerful and efficient AI applications.\nWe will cover the full stack of AI systems, from low-level kernel optimizations to high-level algorithm and model designs. The course will survey and dissect recent influential research papers in the field.\nTopics include:\nKernel-Level Optimizations: I/O-aware and exact attention mechanisms (e.g., FlashAttention), sparse attention, and custom kernel generation with AI compilers (e.g., TVM, MLIR).\nEfficient LLMs: State-of-the-art techniques for efficient training (e.g., ZeRO, LoRA), inference (e.g., vLLM, speculative decoding), model compression (quantization, pruning), and long-context optimizations.\nEfficient Model Architectures: Design principles for efficient models, including Mixture-of-Experts (MoE) and State Space Models (Mamba).\nGenerative AI Systems: Optimization techniques for emerging modalities, including efficient video generation, super-resolution, and understanding.\nSecure and Private AI: Methods for ensuring model and data security, including watermarking and encryption in the context of large models.models\nThis course is run as a research seminar. The focus will be on critically reading, presenting, and discussing influential papers in efficient AI systems. Through weekly readings and student-led presentations, participants will gain a deep understanding of the key challenges, foundational techniques, and future directions in the field. Approximately every two weeks, we will also host a guest lecture from a leading researcher or engineer from academia or industry, providing direct insights into the state-of-the-art.\nLecture Videos: [Link to be provided]\nTime: Friday 3:00 PM–4:15 PM CST\nLocation: [TBA]\nOffice Hour: [TBA]\nDiscussion: [TBA]\nHomework Submission: [TBA]\nContact:\n[TBD]\nPrerequisites:\n[TBD]\n","description":" COMP 620 - Fall 2025 This course is a research seminar focused on efficient machine learning systems. This is a crucial area as modern deep neural networks, especially Large Language Models (LLMs) and generative models, demand extraordinary levels of computation. This computational cost hinders their deployment and scaling, creating significant challenges for both resource-constrained edge devices and large-scale cloud infrastructure. This course delves into the cutting-edge techniques that enable powerful and efficient AI applications.\n"},{"id":20,"href":"/MLSys-Seminar/materials/","title":"Materials","parent":"Machine Learning System Seminar","content":" Chapter I: Kernel Related Subtopic 1: I/O Aware \u0026amp; Exact Attention Paper Link FlashAttention 1, 2, 3 PDF, PDF, PDF PagedAttention (vLLM) PDF SGLang PDF FlexAttention PDF FlashInfer PDF SpargeAttention PDF SageAttention 1,2 PDF, PDF Subtopic 2: Sparse Attention Paper Link DejaVu PDF H2O PDF SpAttn PDF MoE PDF Deepspeed-MoE PDF Subtopic 3: Kernel Generation \u0026amp; Compiler Paper Link TVM PDF Ansor PDF MLIR PDF Subtopic 4: Execution Optimization/Serving Paper Link Alpa PDF Orca PDF FlexGen PDF ZeRO-Offloading PDF Megatron-LM PDF FlashDecoding++ PDF SarathiServe PDF Chapter II: Efficient LLM Subtopic 1: LLM 101 Paper Link Attention is All You Need PDF BERT PDF GPT-3 PDF Scaling Laws PDF RLHF PDF PPO/DPO PDF , PDF Subtopic 2: Efficient Inference \u0026amp; Long-context Paper Link Streaming LLM \u0026amp; DuoAttention PDF, PDF MInference PDF H2O PDF TOVA/KIVI PDF, PDF Speculative Decoding PDF, PDF Multi-token prediction: Deepseek-v3 PDF Subtopic 3: Model Compression (Quant \u0026amp; Pruning) Paper Link LLM.int8()/GPTQ PDF, PDF AWQ PDF LLM Pruner PDF ShearedLlama PDF Subtopic 4: Efficient Training Paper Link ZeRO PDF Megatron-LM PDF LoRA \u0026amp; QLoRA PDF, PDF Subtopic 5: Efficient Model Designs Paper Link Switch Transformers/Outrageously Large Neural Networks PDF, PDF MLA Attention PDF Mamba PDF Chapter III: Video Generation Subtopic 1: SOTA/Baseline Model Paper Link CogVideoX PDF HunyuanVideo PDF WAN PDF Seaweed-7B PDF Subtopic 2: Optimization Techniques Paper Link Pruning PDF Cache PDF Compression PDF Sparsity PDF Subtopic 3: Long Video Generation Paper Link Tuning-Free Multi-Event Long Video Generation PDF Long Context Tuning for Video Generation PDF One-Minute Video Generation with Test-Time Training PDF SKYREELS-V2 PDF Subtopic 4: Video Super Resolution Paper Link SeedVR PDF MGLD-VSR PDF DynamicScaler PDF Chapter IV: Secure LLM Subtopic 1: Diffusion Model/Flow Matching Subtopic 2: Watermarking Subtopic 3: Efficient CNN Subtopic 4: Encryption Chapter V: MLLM Video Understanding Subtopic 1: SOTA/Baseline Subtopic 2: Optimization Techniques Subtopic 3: Algorithm Design ","description":" Chapter I: Kernel Related Subtopic 1: I/O Aware \u0026amp; Exact Attention Paper Link FlashAttention 1, 2, 3 PDF, PDF, PDF PagedAttention (vLLM) PDF SGLang PDF FlexAttention PDF FlashInfer PDF SpargeAttention PDF SageAttention 1,2 PDF, PDF Subtopic 2: Sparse Attention Paper Link DejaVu PDF H2O PDF SpAttn PDF MoE PDF Deepspeed-MoE PDF Subtopic 3: Kernel Generation \u0026amp; Compiler Paper Link TVM PDF Ansor PDF MLIR PDF Subtopic 4: Execution Optimization/Serving Paper Link Alpa PDF Orca PDF FlexGen PDF ZeRO-Offloading PDF Megatron-LM PDF FlashDecoding++ PDF SarathiServe PDF Chapter II: Efficient LLM Subtopic 1: LLM 101 Paper Link Attention is All You Need PDF BERT PDF GPT-3 PDF Scaling Laws PDF RLHF PDF PPO/DPO PDF , PDF Subtopic 2: Efficient Inference \u0026amp; Long-context Paper Link Streaming LLM \u0026amp; DuoAttention PDF, PDF MInference PDF H2O PDF TOVA/KIVI PDF, PDF Speculative Decoding PDF, PDF Multi-token prediction: Deepseek-v3 PDF Subtopic 3: Model Compression (Quant \u0026amp; Pruning) Paper Link LLM.int8()/GPTQ PDF, PDF AWQ PDF LLM Pruner PDF ShearedLlama PDF Subtopic 4: Efficient Training Paper Link ZeRO PDF Megatron-LM PDF LoRA \u0026amp; QLoRA PDF, PDF Subtopic 5: Efficient Model Designs Paper Link Switch Transformers/Outrageously Large Neural Networks PDF, PDF MLA Attention PDF Mamba PDF Chapter III: Video Generation Subtopic 1: SOTA/Baseline Model Paper Link CogVideoX PDF HunyuanVideo PDF WAN PDF Seaweed-7B PDF Subtopic 2: Optimization Techniques Paper Link Pruning PDF Cache PDF Compression PDF Sparsity PDF Subtopic 3: Long Video Generation Paper Link Tuning-Free Multi-Event Long Video Generation PDF Long Context Tuning for Video Generation PDF One-Minute Video Generation with Test-Time Training PDF SKYREELS-V2 PDF Subtopic 4: Video Super Resolution Paper Link SeedVR PDF MGLD-VSR PDF DynamicScaler PDF Chapter IV: Secure LLM Subtopic 1: Diffusion Model/Flow Matching Subtopic 2: Watermarking Subtopic 3: Efficient CNN Subtopic 4: Encryption Chapter V: MLLM Video Understanding Subtopic 1: SOTA/Baseline Subtopic 2: Optimization Techniques Subtopic 3: Algorithm Design "},{"id":21,"href":"/MLSys-Seminar/staffers/","title":"Staff","parent":"Machine Learning System Seminar","content":" ","description":""},{"id":22,"href":"/MLSys-Seminar/tags/","title":"Tags","parent":"Machine Learning System Seminar","content":"","description":""},{"id":23,"href":"/MLSys-Seminar/toc-tree/","title":"ToC-Tree","parent":"Machine Learning System Seminar","content":"This is just a demo section for the toc-tree shortcode.\nLevel 1 Level 1.1 Level 1.2 Level 1.3 Level 1.3.1 Level 2 Level 2.1 Level 2.2 ","description":"This is just a demo section for the toc-tree shortcode.\nLevel 1 Level 1.1 Level 1.2 Level 1.3 Level 1.3.1 Level 2 Level 2.1 Level 2.2 "}]