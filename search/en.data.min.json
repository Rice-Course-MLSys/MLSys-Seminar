[{"id":0,"href":"/MLSys-Seminar/collapse/level-1/","title":"Level 1","parent":"Collapse","content":"Level 1\nLevel 1.1 Level 1.2 ","description":"Level 1\nLevel 1.1 Level 1.2 "},{"id":1,"href":"/MLSys-Seminar/collapse/level-2/","title":"Level 2","parent":"Collapse","content":"Level-2\n","description":"Level-2\n"},{"id":2,"href":"/MLSys-Seminar/staffers/fanjiang/","title":"Fanjiang","parent":"Staff","content":"","description":""},{"id":3,"href":"/MLSys-Seminar/staffers/yuke/","title":"Yuke","parent":"Staff","content":"","description":""},{"id":4,"href":"/MLSys-Seminar/toc-tree/level-1/","title":"Level 1","parent":"ToC-Tree","content":"Level 1\nLevel 1.1 Level 1.2 Level 1.3 Level 1.3.1 ","description":"Level 1\nLevel 1.1 Level 1.2 Level 1.3 Level 1.3.1 "},{"id":5,"href":"/MLSys-Seminar/toc-tree/level-2/","title":"Level 2","parent":"ToC-Tree","content":"Level-2\n","description":"Level-2\n"},{"id":6,"href":"/MLSys-Seminar/about/","title":"About","parent":"TinyML and Efficient Deep Learning Computing","content":"This course focuses on efficient machine learning and systems. This is a crucial area as deep neural networks demand extraordinary levels of computation, hindering its deployment on everyday devices and burdening the cloud infrastructure. This course introduces efficient AI computing techniques that enable powerful deep learning applications on resource-constrained devices.\nTopics include:\nModel compression, pruning, quantization Neural architecture search Distributed training Data/model parallelism Gradient compression On-device fine-tuning Application-specific acceleration techniques for large language models and diffusion models Students will get hands-on experience implementing model compression techniques and deploying large language models (Llama2-7B) on a laptop.\nLecture Videos: https://live.efficientml.ai/\nTime: Tuesday/Thursday 3:35-5:00pm Eastern Time\nLocation: 34-101\nOffice Hour: Thursday 5:00-6:00 pm Eastern Time, 38-344 Meeting Room\nDiscussion: Piazza\nHomework Submission: Canvas\nContact:\nFor external inquiries, personal matters, or emergencies, you can email us at efficientml-staff [at] mit.edu. If you are interested in getting updates, please sign up here to join our mailing list to get notified!\nPrerequisites: 6.191 Computation Structures and 6.390 Intro to Machine Learning. Students who don\u0026rsquo;t full-fill the prerequisites will be de-registered in the second week of class. If you believe you have equivalent prior experience (e.g., a computer architecture course taken during your undergraduate studies at another institution), you may petition for consideration. Please submit your Petition Form by Sept. 6, 2024, 11:59:59 PM EST.\n","description":"This course focuses on efficient machine learning and systems. This is a crucial area as deep neural networks demand extraordinary levels of computation, hindering its deployment on everyday devices and burdening the cloud infrastructure. This course introduces efficient AI computing techniques that enable powerful deep learning applications on resource-constrained devices.\nTopics include:\nModel compression, pruning, quantization Neural architecture search Distributed training Data/model parallelism Gradient compression On-device fine-tuning Application-specific acceleration techniques for large language models and diffusion models Students will get hands-on experience implementing model compression techniques and deploying large language models (Llama2-7B) on a laptop.\n"},{"id":7,"href":"/MLSys-Seminar/collapse/","title":"Collapse","parent":"TinyML and Efficient Deep Learning Computing","content":"Demo collapsible menu entries.\n","description":"Demo collapsible menu entries.\n"},{"id":8,"href":"/MLSys-Seminar/collapse/level-1/level-1-1/","title":"Level 1.1","parent":"Level 1","content":"Level 1.1\n","description":"Level 1.1\n"},{"id":9,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-1/","title":"Level 1.1","parent":"Level 1","content":"Level 1.1\n","description":"Level 1.1\n"},{"id":10,"href":"/MLSys-Seminar/collapse/level-1/level-1-2/","title":"Level 1.2","parent":"Level 1","content":"Level 1.2\n","description":"Level 1.2\n"},{"id":11,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-2/","title":"Level 1.2","parent":"Level 1","content":"Level 1.2\n","description":"Level 1.2\n"},{"id":12,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-3/","title":"Level 1.3","parent":"Level 1","content":"Level 1.3\nLevel 1.3.1 ","description":"Level 1.3\nLevel 1.3.1 "},{"id":13,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-3/level-1-3-1/","title":"Level 1.3.1","parent":"Level 1.3","content":"Level 1.3.1\n","description":"Level 1.3.1\n"},{"id":14,"href":"/MLSys-Seminar/collapse/level-2/level-2-1/","title":"Level 2.1","parent":"Level 2","content":"Level 2.1\n","description":"Level 2.1\n"},{"id":15,"href":"/MLSys-Seminar/toc-tree/level-2/level-2-1/","title":"Level 2.1","parent":"Level 2","content":"Level 2.1\n","description":"Level 2.1\n"},{"id":16,"href":"/MLSys-Seminar/collapse/level-2/level-2-2/","title":"Level 2.2","parent":"Level 2","content":"Level 2.2\n","description":"Level 2.2\n"},{"id":17,"href":"/MLSys-Seminar/toc-tree/level-2/level-2-2/","title":"Level 2.2","parent":"Level 2","content":"Level 2.2\n","description":"Level 2.2\n"},{"id":18,"href":"/MLSys-Seminar/logistics/","title":"Logistics","parent":"TinyML and Efficient Deep Learning Computing","content":" Grading The class requirements include five labs, and one final project. This is a PhD level course, and by the end of this class you should have a good understanding of efficient deep learning techniques, and be able to deploy large language models (LLMs) on your laptop.\nThe grading breakdown is as follows:\n5 Labs (15% x 5) Final Project (25%) Proposal (5%) Presentation + Final Report (20%) Participation Bonus (4%) Note that this class does not have any tests or exams.\nLabs There will be 5 labs over the course of the semester.\nLab1: Pruning Lab2: Quantization Lab3: Neural architecture search Lab4: LLM compression Lab5: LLM deployment on laptop Collaboration Policy Labs must be done individually: each student must hand in their own answers. However, it is acceptable to collaborate when figuring out answers and to help each other solve the problems. We will be assuming that, as participants in a graduate course, you will be taking the responsibility to make sure you personally understand the solution arising from such collaboration. You also must indicate on each homework with whom you have collaborated.\nLate Policy You will be allowed 6 total homework late days without penalty for the entire semester. You may be late by up to 6 days on any homework assignment. Once those days are used, you will be penalized according to the following policy:\nHomework is worth full credit at the due time on the due date. The allowed late days are counted by day (i.e., each new late day starts at 11:59 pm ET). Once the allowed late days are exceeded, the penalty is 50% per late day counted by day. The homework is worth zero credit 2 days after exceeding the late day limit. You must turn in at least 4 of the 5 assignments, even if for zero credit, in order to pass the course. Regrade Policy If you feel that we have made a mistake in grading your work, please submit a regrading request to TAs during the office hour and we will consider your request. Please note that regrading of a homework may cause your grade to go either up or down.\nFinal Project The class project will be carried out in groups of 4 or 5 people, and has three main parts:\nProposal: choose from a list of suggested projects, or propose your own project Poster presentation Final report (4 pages, using the NeurIPS template) Participation Bonus We appreciate everyone being actively involved in the class! Around the end of the semester, we will send out a survey to help us understand how the course is going, and how we can improve. Completing it is worth 4% in total.\n","description":" Grading The class requirements include five labs, and one final project. This is a PhD level course, and by the end of this class you should have a good understanding of efficient deep learning techniques, and be able to deploy large language models (LLMs) on your laptop.\nThe grading breakdown is as follows:\n5 Labs (15% x 5) Final Project (25%) Proposal (5%) Presentation + Final Report (20%) Participation Bonus (4%) Note that this class does not have any tests or exams.\n"},{"id":19,"href":"/MLSys-Seminar/materials/","title":"Materials","parent":"TinyML and Efficient Deep Learning Computing","content":" Chapter I: Kernel Related Subtopic 1: I/O Aware \u0026amp; Exact Attention Paper Link FlashAttention 1, 2, 3 PDF, PDF, PDF PagedAttention (vLLM) PDF SGLang PDF FlexAttention PDF FlashInfer PDF SpargeAttention PDF SageAttention 1,2 PDF, PDF Subtopic 2: Sparse Attention Paper Link DejaVu PDF H2O PDF SpAttn PDF MoE PDF Deepseek-MoE PDF Subtopic 3: Kernel Generation \u0026amp; Compiler Paper Link TVM PDF Ansor PDF MLIR PDF Subtopic 4: Execution Optimization/Serving Paper Link Alpa PDF Orca PDF FlexGen PDF ZeRO-Offloading PDF Megatron-LM PDF FlashDecoding++ PDF SarathiServe PDF Chapter II: Efficient LLM Subtopic 1: LLM 101 Paper Link Attention is All You Need PDF BERT PDF GPT-3 PDF Scaling Laws PDF RLHF PDF PPO/DPO PDF , PDF Subtopic 2: Efficient Inference \u0026amp; Long-context Paper Link Streaming LLM \u0026amp; DuoAttention PDF, PDF MInference PDF H2O PDF TOVA/KIVI PDF, PDF Speculative Decoding PDF, PDF Multi-token prediction: Deepseek-v3 PDF Subtopic 3: Model Compression (Quant \u0026amp; Pruning) Paper Link LLM.int8()/GPTQ PDF, PDF AWQ PDF LLM Pruner PDF ShearedLlama PDF Subtopic 4: Efficient Training Paper Link ZeRO PDF Megatron-LM PDF LoRA \u0026amp; QLoRA PDF, PDF Subtopic 5: Efficient Model Designs Paper Link Swtich Transformers/Outrageously Large Neural Networks PDF, PDF MLA Attention PDF Mamba PDF Chapter III: Video Generation Subtopic 1: SOTA/Baseline Model Paper Link CogVideoX PDF HunyuanVideo PDF WAN PDF Seaweed-7B PDF Subtopic 2: Optimization Techniques Paper Link Pruning UniCP Cache PDF, need to add more.. Compression PDF Sparsity PDF Subtopic 3: Long Video Generation Paper Link Tuning-Free Multi-Event Long Video Generation PDF Long Context Tuning for Video Generation PDF One-Minute Video Generation with Test-Time Training PDF SKYREELS-V2: INFINITE-LENGTH FILM GENERATIVE MODEL PDF Subtopic 4: Video Super Resolution Paper Link SeedVR PDF MGLD-VSR PDF DynamicScaler PDF Chapter IV: Secure LLM Subtopic 1: Diffusion Model/Flow Matching Subtopic 2: Watermarking Subtopic 3: Efficient CNN Subtopic 4: Encryption Chapter V: MLLM Video Understanding Subtopic 1: SOTA/Baseline Subtopic 2: Optimization Techniques Subtopic 3: Algorithm Design ","description":" Chapter I: Kernel Related Subtopic 1: I/O Aware \u0026amp; Exact Attention Paper Link FlashAttention 1, 2, 3 PDF, PDF, PDF PagedAttention (vLLM) PDF SGLang PDF FlexAttention PDF FlashInfer PDF SpargeAttention PDF SageAttention 1,2 PDF, PDF Subtopic 2: Sparse Attention Paper Link DejaVu PDF H2O PDF SpAttn PDF MoE PDF Deepseek-MoE PDF Subtopic 3: Kernel Generation \u0026amp; Compiler Paper Link TVM PDF Ansor PDF MLIR PDF Subtopic 4: Execution Optimization/Serving Paper Link Alpa PDF Orca PDF FlexGen PDF ZeRO-Offloading PDF Megatron-LM PDF FlashDecoding++ PDF SarathiServe PDF Chapter II: Efficient LLM Subtopic 1: LLM 101 Paper Link Attention is All You Need PDF BERT PDF GPT-3 PDF Scaling Laws PDF RLHF PDF PPO/DPO PDF , PDF Subtopic 2: Efficient Inference \u0026amp; Long-context Paper Link Streaming LLM \u0026amp; DuoAttention PDF, PDF MInference PDF H2O PDF TOVA/KIVI PDF, PDF Speculative Decoding PDF, PDF Multi-token prediction: Deepseek-v3 PDF Subtopic 3: Model Compression (Quant \u0026amp; Pruning) Paper Link LLM.int8()/GPTQ PDF, PDF AWQ PDF LLM Pruner PDF ShearedLlama PDF Subtopic 4: Efficient Training Paper Link ZeRO PDF Megatron-LM PDF LoRA \u0026amp; QLoRA PDF, PDF Subtopic 5: Efficient Model Designs Paper Link Swtich Transformers/Outrageously Large Neural Networks PDF, PDF MLA Attention PDF Mamba PDF Chapter III: Video Generation Subtopic 1: SOTA/Baseline Model Paper Link CogVideoX PDF HunyuanVideo PDF WAN PDF Seaweed-7B PDF Subtopic 2: Optimization Techniques Paper Link Pruning UniCP Cache PDF, need to add more.. Compression PDF Sparsity PDF Subtopic 3: Long Video Generation Paper Link Tuning-Free Multi-Event Long Video Generation PDF Long Context Tuning for Video Generation PDF One-Minute Video Generation with Test-Time Training PDF SKYREELS-V2: INFINITE-LENGTH FILM GENERATIVE MODEL PDF Subtopic 4: Video Super Resolution Paper Link SeedVR PDF MGLD-VSR PDF DynamicScaler PDF Chapter IV: Secure LLM Subtopic 1: Diffusion Model/Flow Matching Subtopic 2: Watermarking Subtopic 3: Efficient CNN Subtopic 4: Encryption Chapter V: MLLM Video Understanding Subtopic 1: SOTA/Baseline Subtopic 2: Optimization Techniques Subtopic 3: Algorithm Design "},{"id":20,"href":"/MLSys-Seminar/schedule/","title":"Schedule","parent":"TinyML and Efficient Deep Learning Computing","content":" Date Lecture Logistics Sep 5 Lecture 1: Introduction\n[Slides] [Video] Sep 10 Lecture 2: Basics of Deep Learning\n[Slides] [Video] [Video (Live)] Lab 0 out Chapter I: Efficient Inference Sep 12 Lecture 3: Pruning and Sparsity (Part I)\n[Slides] [Video] [Video (Live)] ","description":" Date Lecture Logistics Sep 5 Lecture 1: Introduction\n[Slides] [Video] Sep 10 Lecture 2: Basics of Deep Learning\n[Slides] [Video] [Video (Live)] Lab 0 out Chapter I: Efficient Inference Sep 12 Lecture 3: Pruning and Sparsity (Part I)\n[Slides] [Video] [Video (Live)] "},{"id":21,"href":"/MLSys-Seminar/staffers/","title":"Staff","parent":"TinyML and Efficient Deep Learning Computing","content":" ","description":""},{"id":22,"href":"/MLSys-Seminar/tags/","title":"Tags","parent":"TinyML and Efficient Deep Learning Computing","content":"","description":""},{"id":23,"href":"/MLSys-Seminar/","title":"TinyML and Efficient Deep Learning Computing","parent":"","content":" CS 598 - Fall 2025 This course focuses on efficient machine learning and systems. This is a crucial area as deep neural networks demand extraordinary levels of computation, hindering its deployment on everyday devices and burdening the cloud infrastructure. This course introduces efficient AI computing techniques that enable powerful deep learning applications on resource-constrained devices.\nTopics include:\nModel compression, pruning, quantization Neural architecture search Distributed training Data/model parallelism Gradient compression On-device fine-tuning Application-specific acceleration techniques for large language models and diffusion models Students will get hands-on experience implementing model compression techniques and deploying large language models (Llama2-7B) on a laptop.\nLecture Videos: https://live.efficientml.ai/\nTime: Tuesday/Thursday 3:35-5:00pm Eastern Time\nLocation: 34-101\nOffice Hour: Thursday 5:00-6:00 pm Eastern Time, 38-344 Meeting Room\nDiscussion: Piazza\nHomework Submission: Canvas\nContact:\nFor external inquiries, personal matters, or emergencies, you can email us at efficientml-staff [at] mit.edu. If you are interested in getting updates, please sign up here to join our mailing list to get notified!\nPrerequisites: 6.191 Computation Structures and 6.390 Intro to Machine Learning. Students who don\u0026rsquo;t full-fill the prerequisites will be de-registered in the second week of class. If you believe you have equivalent prior experience (e.g., a computer architecture course taken during your undergraduate studies at another institution), you may petition for consideration. Please submit your Petition Form by Sept. 6, 2024, 11:59:59 PM EST.\n","description":" CS 598 - Fall 2025 This course focuses on efficient machine learning and systems. This is a crucial area as deep neural networks demand extraordinary levels of computation, hindering its deployment on everyday devices and burdening the cloud infrastructure. This course introduces efficient AI computing techniques that enable powerful deep learning applications on resource-constrained devices.\n"},{"id":24,"href":"/MLSys-Seminar/toc-tree/","title":"ToC-Tree","parent":"TinyML and Efficient Deep Learning Computing","content":"This is just a demo section for the toc-tree shortcode.\nLevel 1 Level 1.1 Level 1.2 Level 1.3 Level 1.3.1 Level 2 Level 2.1 Level 2.2 ","description":"This is just a demo section for the toc-tree shortcode.\nLevel 1 Level 1.1 Level 1.2 Level 1.3 Level 1.3.1 Level 2 Level 2.1 Level 2.2 "}]