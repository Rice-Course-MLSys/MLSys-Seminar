[{"id":0,"href":"/MLSys-Seminar/collapse/level-1/","title":"Level 1","parent":"Collapse","content":"Level 1\nLevel 1.1 Level 1.2 ","description":"Level 1\nLevel 1.1 Level 1.2 "},{"id":1,"href":"/MLSys-Seminar/collapse/level-2/","title":"Level 2","parent":"Collapse","content":"Level-2\n","description":"Level-2\n"},{"id":2,"href":"/MLSys-Seminar/staffers/yuke/","title":"Yuke","parent":"Staff","content":"","description":""},{"id":3,"href":"/MLSys-Seminar/toc-tree/level-1/","title":"Level 1","parent":"ToC-Tree","content":"Level 1\nLevel 1.1 Level 1.2 Level 1.3 Level 1.3.1 ","description":"Level 1\nLevel 1.1 Level 1.2 Level 1.3 Level 1.3.1 "},{"id":4,"href":"/MLSys-Seminar/toc-tree/level-2/","title":"Level 2","parent":"ToC-Tree","content":"Level-2\n","description":"Level-2\n"},{"id":5,"href":"/MLSys-Seminar/about/","title":"About","parent":"Machine Learning System Seminar","content":" [IMPORTANT UPDATE] Assignment Schedule: [Google Sheet]\nThis course is a research seminar focused on efficient machine learning systems. This is a crucial area as modern deep neural networks, especially Large Language Models (LLMs) and generative models, demand extraordinary levels of computation. This computational cost hinders their deployment and scaling, creating significant challenges for both resource-constrained edge devices and large-scale cloud infrastructure. This course delves into the cutting-edge techniques that enable powerful and efficient AI applications.\nWe will cover the full stack of AI systems, from low-level kernel optimizations to high-level algorithm and model designs. The course will survey and dissect recent influential research papers in the field.\nTopics include:\nKernel-Level Optimizations: I/O-aware and exact attention mechanisms (e.g., FlashAttention), sparse attention, and custom kernel generation with AI compilers (e.g., TVM, MLIR).\nEfficient LLMs: State-of-the-art techniques for efficient training (e.g., ZeRO, LoRA), inference (e.g., vLLM, speculative decoding), model compression (quantization, pruning), and long-context optimizations.\nEfficient Model Architectures: Design principles for efficient models, including Mixture-of-Experts (MoE) and State Space Models (Mamba).\nGenerative AI Systems: Optimization techniques for emerging modalities, including efficient video generation, super-resolution, and understanding.\nSecure and Private AI: Methods for ensuring model and data security, including watermarking and encryption in the context of large models.models\nThis course is run as a research seminar. The focus will be on critically reading, presenting, and discussing influential papers in efficient AI systems. Through weekly readings and student-led presentations, participants will gain a deep understanding of the key challenges, foundational techniques, and future directions in the field. Approximately every two weeks, we will also host a guest lecture from a leading researcher or engineer from academia or industry, providing direct insights into the state-of-the-art.\nLecture Videos: [Link to be provided]\nTime: Friday 3:00 PM–4:15 PM CST\nBuilding: Anne and Charles Duncan Hall\nRoom: 1070\nStart Date: 08/25/2025\nEnd Date: 12/05/2025\n","description":" [IMPORTANT UPDATE] Assignment Schedule: [Google Sheet]\nThis course is a research seminar focused on efficient machine learning systems. This is a crucial area as modern deep neural networks, especially Large Language Models (LLMs) and generative models, demand extraordinary levels of computation. This computational cost hinders their deployment and scaling, creating significant challenges for both resource-constrained edge devices and large-scale cloud infrastructure. This course delves into the cutting-edge techniques that enable powerful and efficient AI applications.\n"},{"id":6,"href":"/MLSys-Seminar/collapse/","title":"Collapse","parent":"Machine Learning System Seminar","content":"Demo collapsible menu entries.\n","description":"Demo collapsible menu entries.\n"},{"id":7,"href":"/MLSys-Seminar/schedule/","title":"Guest Lectures","parent":"Machine Learning System Seminar","content":" Date Lecturer Topic Aug 293:00 PM–3:40 PM Yuke Wang [Slides] Introduction Sep 53:00 PM–3:40 PM Boyuan Feng [Slides] BioBoyuan Feng is a PyTorch Core Developer working on PyTorch Compiler, Inductor, CUDAGraph, and Flex Attention.Lecture AbstractFlexAttention is a novel compiler-driven programming model that allows implementing the majority of attention variants in a few lines of idiomatic PyTorch code. Since its release in PyTorch 2.5.0, many ML researchers have utilized it to customize their attention kernels without writing kernel code. In this talk, we present recent advances in FlexAttention. More details on our MLSys'25 paper (https://arxiv.org/pdf/2412.05496) and PyTorch Blog (https://pytorch.org/blog/flexattention-for-inference/)! FlexAttention Sep 193:00 PM–3:40 PM Yue Guan [Slides] BioYue Guan is a postdoctoral researcher at the University of California, San Diego, working with Prof. Yufei Ding in the Picasso Lab. He received his Ph.D. in Computer Science from Shanghai Jiao Tong University under the supervision of Prof. Jingwen Leng. His research focuses on efficient deep learning systems, spanning model compression, compiler optimization, and system design. His work has been published in top venues such as SOSP, OSDI, ASPLOS and HPCA.Lecture AbstractThe rapid growth of large language models (LLMs) requires better compilers for efficient use of multi-GPU systems. In this talk, I will introduce Mercury, a compiler that manages remote GPU memory as part of the memory hierarchy to optimize computation, storage, and communication. I will also present KPerfIR, a tool that adds profiling directly into the compilation process to help analyze GPU kernel performance. These approaches show how integrating optimization and performance analysis in compilers can improve the scalability and efficiency of LLMs. Mercucy \u0026amp; KPerfIR Oct 173:00 PM–3:40 PM Liangyu Zhao [Slides] BioLiangyu Zhao is a fourth-year PhD student at the University of Washington, advised by Prof. Arvind Krishnamurthy. His research focuses on machine learning systems, with an emphasis on network communication for distributed machine learning. Currently, he is a research scientist intern at Meta AI \u0026amp; Systems Co-Design team.Lecture AbstractAs modern DNN models grow ever larger, collective communications between the accelerators (allreduce, etc.) emerge as a significant performance bottleneck. Designing efficient communication schedules is challenging, given today\u0026rsquo;s heterogeneous and diverse network fabrics. We present ForestColl, a tool that generates throughput-optimal schedules for any network topology. ForestColl constructs broadcast/aggregation spanning trees as the communication schedule, achieving theoretical optimality. Its schedule generation runs in strongly polynomial time and is highly scalable. ForestColl supports any network fabrics, including both switching fabrics and direct accelerator connections. We evaluated ForestColl on multi-box AMD MI250 and NVIDIA DGX A100 platforms. ForestColl showed significant improvements over the vendors\u0026rsquo; own optimized communication libraries, RCCL and NCCL, across various settings and in LLM training. ForestColl also outperformed other state-of-the-art schedule generation techniques with both more efficient generated schedules and substantially faster schedule generation speed. ForestColl Oct 313:00 PM–3:40 PM Zhuang Wang [Slides] BioZhuang Wang is an Applied Scientist at Amazon Web Services AI. He received his Ph.D. degree in Computer Science from Rice University in 2023, fortunately advised by Prof. T. S. Eugene Ng. His current research interests focus on efficient training and inference systems for large language models.Lecture AbstractFrequent failures are observed during large model training due to large-scale resources involved and extended training time. This talk presents Gemini, a distributed training system that enables fast failure recovery for large model training by checkpointing to CPU memory of the host machines with much larger aggregated bandwidth. However, two challenges prevent naïvely checkpointing to CPU memory. First, the availability of checkpoints in CPU memory cannot be guaranteed when failures occur. Second, since the communication traffic for training and checkpointing share the same network, checkpoint traffic can interfere with training traffic and harm training throughput. To address these two challenges, we propose: 1) a provably near-optimal checkpoint placement strategy to maximize the probability of failure recovery from checkpoints in CPU memory; and 2) a checkpoint traffic scheduling algorithm to minimize, if not eliminate, the interference of checkpoint traffic on model training. Our evaluation shows that Gemini achieves optimal checkpoint frequency, i.e., every iteration, and incurs no overhead on training throughput for large model training. Gemini Nov 143:00 PM–3:40 PM Yixin Dong [Slides] BioTo Be Finished Lecture AbstractTopic Not Confirmed [TBD] ","description":" Date Lecturer Topic Aug 293:00 PM–3:40 PM Yuke Wang [Slides] Introduction Sep 53:00 PM–3:40 PM Boyuan Feng [Slides] BioBoyuan Feng is a PyTorch Core Developer working on PyTorch Compiler, Inductor, CUDAGraph, and Flex Attention.Lecture AbstractFlexAttention is a novel compiler-driven programming model that allows implementing the majority of attention variants in a few lines of idiomatic PyTorch code. Since its release in PyTorch 2.5.0, many ML researchers have utilized it to customize their attention kernels without writing kernel code. In this talk, we present recent advances in FlexAttention. More details on our MLSys'25 paper (https://arxiv.org/pdf/2412.05496) and PyTorch Blog (https://pytorch.org/blog/flexattention-for-inference/)! FlexAttention Sep 193:00 PM–3:40 PM Yue Guan [Slides] BioYue Guan is a postdoctoral researcher at the University of California, San Diego, working with Prof. Yufei Ding in the Picasso Lab. He received his Ph.D. in Computer Science from Shanghai Jiao Tong University under the supervision of Prof. Jingwen Leng. His research focuses on efficient deep learning systems, spanning model compression, compiler optimization, and system design. His work has been published in top venues such as SOSP, OSDI, ASPLOS and HPCA.Lecture AbstractThe rapid growth of large language models (LLMs) requires better compilers for efficient use of multi-GPU systems. In this talk, I will introduce Mercury, a compiler that manages remote GPU memory as part of the memory hierarchy to optimize computation, storage, and communication. I will also present KPerfIR, a tool that adds profiling directly into the compilation process to help analyze GPU kernel performance. These approaches show how integrating optimization and performance analysis in compilers can improve the scalability and efficiency of LLMs. Mercucy \u0026amp; KPerfIR Oct 173:00 PM–3:40 PM Liangyu Zhao [Slides] BioLiangyu Zhao is a fourth-year PhD student at the University of Washington, advised by Prof. Arvind Krishnamurthy. His research focuses on machine learning systems, with an emphasis on network communication for distributed machine learning. Currently, he is a research scientist intern at Meta AI \u0026amp; Systems Co-Design team.Lecture AbstractAs modern DNN models grow ever larger, collective communications between the accelerators (allreduce, etc.) emerge as a significant performance bottleneck. Designing efficient communication schedules is challenging, given today\u0026rsquo;s heterogeneous and diverse network fabrics. We present ForestColl, a tool that generates throughput-optimal schedules for any network topology. ForestColl constructs broadcast/aggregation spanning trees as the communication schedule, achieving theoretical optimality. Its schedule generation runs in strongly polynomial time and is highly scalable. ForestColl supports any network fabrics, including both switching fabrics and direct accelerator connections. We evaluated ForestColl on multi-box AMD MI250 and NVIDIA DGX A100 platforms. ForestColl showed significant improvements over the vendors\u0026rsquo; own optimized communication libraries, RCCL and NCCL, across various settings and in LLM training. ForestColl also outperformed other state-of-the-art schedule generation techniques with both more efficient generated schedules and substantially faster schedule generation speed. ForestColl Oct 313:00 PM–3:40 PM Zhuang Wang [Slides] BioZhuang Wang is an Applied Scientist at Amazon Web Services AI. He received his Ph.D. degree in Computer Science from Rice University in 2023, fortunately advised by Prof. T. S. Eugene Ng. His current research interests focus on efficient training and inference systems for large language models.Lecture AbstractFrequent failures are observed during large model training due to large-scale resources involved and extended training time. This talk presents Gemini, a distributed training system that enables fast failure recovery for large model training by checkpointing to CPU memory of the host machines with much larger aggregated bandwidth. However, two challenges prevent naïvely checkpointing to CPU memory. First, the availability of checkpoints in CPU memory cannot be guaranteed when failures occur. Second, since the communication traffic for training and checkpointing share the same network, checkpoint traffic can interfere with training traffic and harm training throughput. To address these two challenges, we propose: 1) a provably near-optimal checkpoint placement strategy to maximize the probability of failure recovery from checkpoints in CPU memory; and 2) a checkpoint traffic scheduling algorithm to minimize, if not eliminate, the interference of checkpoint traffic on model training. Our evaluation shows that Gemini achieves optimal checkpoint frequency, i.e., every iteration, and incurs no overhead on training throughput for large model training. Gemini Nov 143:00 PM–3:40 PM Yixin Dong [Slides] BioTo Be Finished Lecture AbstractTopic Not Confirmed [TBD] "},{"id":8,"href":"/MLSys-Seminar/collapse/level-1/level-1-1/","title":"Level 1.1","parent":"Level 1","content":"Level 1.1\n","description":"Level 1.1\n"},{"id":9,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-1/","title":"Level 1.1","parent":"Level 1","content":"Level 1.1\n","description":"Level 1.1\n"},{"id":10,"href":"/MLSys-Seminar/collapse/level-1/level-1-2/","title":"Level 1.2","parent":"Level 1","content":"Level 1.2\n","description":"Level 1.2\n"},{"id":11,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-2/","title":"Level 1.2","parent":"Level 1","content":"Level 1.2\n","description":"Level 1.2\n"},{"id":12,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-3/","title":"Level 1.3","parent":"Level 1","content":"Level 1.3\nLevel 1.3.1 ","description":"Level 1.3\nLevel 1.3.1 "},{"id":13,"href":"/MLSys-Seminar/toc-tree/level-1/level-1-3/level-1-3-1/","title":"Level 1.3.1","parent":"Level 1.3","content":"Level 1.3.1\n","description":"Level 1.3.1\n"},{"id":14,"href":"/MLSys-Seminar/collapse/level-2/level-2-1/","title":"Level 2.1","parent":"Level 2","content":"Level 2.1\n","description":"Level 2.1\n"},{"id":15,"href":"/MLSys-Seminar/toc-tree/level-2/level-2-1/","title":"Level 2.1","parent":"Level 2","content":"Level 2.1\n","description":"Level 2.1\n"},{"id":16,"href":"/MLSys-Seminar/collapse/level-2/level-2-2/","title":"Level 2.2","parent":"Level 2","content":"Level 2.2\n","description":"Level 2.2\n"},{"id":17,"href":"/MLSys-Seminar/toc-tree/level-2/level-2-2/","title":"Level 2.2","parent":"Level 2","content":"Level 2.2\n","description":"Level 2.2\n"},{"id":18,"href":"/MLSys-Seminar/logistics/","title":"Logistics","parent":"Machine Learning System Seminar","content":" Grading The tentative grading breakdown for this course is as follows:\nParticipation: 30% Paper Summary: 30% Paper Presentation \u0026amp; Discussion: 40% Groups Each student will present their own assigned/chosen paper individually (no groups for presentations). Please submit your paper preferences and presentation slot by XXX. After this deadline, the instructor will assign remaining students a paper and a presentation slot. Academic Integrity The Rice Honor Code applies to all activities in this course. All submitted materials (reading responses, presentation slides, etc.) must be your own work. If you reference or use external materials, you must cite them properly. AI Tool Policy Permitted: AI tools may be used for grammar checking and refining initial brainstorms. Not Permitted: The final written content, analyses, and code must be authored by the student. Students are fully responsible for the content they submit and must adhere to the Academic Integrity Policy. ","description":" Grading The tentative grading breakdown for this course is as follows:\nParticipation: 30% Paper Summary: 30% Paper Presentation \u0026amp; Discussion: 40% Groups Each student will present their own assigned/chosen paper individually (no groups for presentations). Please submit your paper preferences and presentation slot by XXX. After this deadline, the instructor will assign remaining students a paper and a presentation slot. Academic Integrity The Rice Honor Code applies to all activities in this course. All submitted materials (reading responses, presentation slides, etc.) must be your own work. If you reference or use external materials, you must cite them properly. AI Tool Policy Permitted: AI tools may be used for grammar checking and refining initial brainstorms. Not Permitted: The final written content, analyses, and code must be authored by the student. Students are fully responsible for the content they submit and must adhere to the Academic Integrity Policy. "},{"id":19,"href":"/MLSys-Seminar/","title":"Machine Learning System Seminar","parent":"","content":" [IMPORTANT UPDATE] Assignment Schedule: [Google Sheet]\n[UPDATE]: Introduction Slides now in Guest Lectures Page\nCOMP 620 - Fall 2025 This course is a research seminar focused on efficient machine learning systems. This is a crucial area as modern deep neural networks, especially Large Language Models (LLMs) and generative models, demand extraordinary levels of computation. This computational cost hinders their deployment and scaling, creating significant challenges for both resource-constrained edge devices and large-scale cloud infrastructure. This course delves into the cutting-edge techniques that enable powerful and efficient AI applications.\nWe will cover the full stack of AI systems, from low-level kernel optimizations to high-level algorithm and model designs. The course will survey and dissect recent influential research papers in the field.\nTopics include:\nKernel-Level Optimizations: I/O-aware and exact attention mechanisms (e.g., FlashAttention), sparse attention, and custom kernel generation with AI compilers (e.g., TVM, MLIR).\nEfficient LLMs: State-of-the-art techniques for efficient training (e.g., ZeRO, LoRA), inference (e.g., vLLM, speculative decoding), model compression (quantization, pruning), and long-context optimizations.\nEfficient Model Architectures: Design principles for efficient models, including Mixture-of-Experts (MoE) and State Space Models (Mamba).\nGenerative AI Systems: Optimization techniques for emerging modalities, including efficient video generation, super-resolution, and understanding.\nSecure and Private AI: Methods for ensuring model and data security, including watermarking and encryption in the context of large models.models\nThis course is run as a research seminar. The focus will be on critically reading, presenting, and discussing influential papers in efficient AI systems. Through weekly readings and student-led presentations, participants will gain a deep understanding of the key challenges, foundational techniques, and future directions in the field. Approximately every two weeks, we will also host a guest lecture from a leading researcher or engineer from academia or industry, providing direct insights into the state-of-the-art.\nLecture Videos: [Link to be provided]\nTime: Friday 3:00 PM–4:15 PM CST\nBuilding: Anne and Charles Duncan Hall\nRoom: 1070\nStart Date: 08/25/2025\nEnd Date: 12/05/2025\n","description":" [IMPORTANT UPDATE] Assignment Schedule: [Google Sheet]\n[UPDATE]: Introduction Slides now in Guest Lectures Page\nCOMP 620 - Fall 2025 This course is a research seminar focused on efficient machine learning systems. This is a crucial area as modern deep neural networks, especially Large Language Models (LLMs) and generative models, demand extraordinary levels of computation. This computational cost hinders their deployment and scaling, creating significant challenges for both resource-constrained edge devices and large-scale cloud infrastructure. This course delves into the cutting-edge techniques that enable powerful and efficient AI applications.\n"},{"id":20,"href":"/MLSys-Seminar/materials/","title":"Materials","parent":"Machine Learning System Seminar","content":" Guest Lecture Materials Paper Link FlexAttention PDF FlexAttention(PyTorch Blog) Website Mercury PDF KPerfIR PDF ForestColl PDF Gemini PDF Chapter I: Kernel Related Subtopic 1: I/O Aware \u0026amp; Exact Attention Paper Link FlashAttention 1, 2, 3 PDF, PDF, PDF PagedAttention (vLLM) PDF SGLang PDF FlexAttention PDF FlashInfer PDF SpargeAttention PDF SageAttention 1,2 PDF, PDF Subtopic 2: Sparse Attention Paper Link DejaVu PDF H2O PDF SpAttn PDF MoE PDF Deepspeed-MoE PDF Subtopic 3: Kernel Generation \u0026amp; Compiler Paper Link TVM PDF Ansor PDF MLIR PDF Subtopic 4: Execution Optimization/Serving Paper Link Alpa PDF Orca PDF FlexGen PDF ZeRO-Offloading PDF Megatron-LM PDF FlashDecoding++ PDF SarathiServe PDF Chapter II: Efficient LLM Subtopic 1: LLM 101 Paper Link Attention is All You Need PDF BERT PDF GPT-3 PDF Scaling Laws PDF RLHF PDF PPO/DPO PDF , PDF Subtopic 2: Efficient Inference \u0026amp; Long-context Paper Link Streaming LLM \u0026amp; DuoAttention PDF, PDF MInference PDF H2O PDF TOVA/KIVI PDF, PDF Speculative Decoding PDF, PDF Multi-token prediction: Deepseek-v3 PDF Subtopic 3: Model Compression (Quant \u0026amp; Pruning) Paper Link LLM.int8()/GPTQ PDF, PDF AWQ PDF LLM Pruner PDF ShearedLlama PDF Subtopic 4: Efficient Training Paper Link ZeRO PDF Megatron-LM PDF LoRA \u0026amp; QLoRA PDF, PDF Subtopic 5: Efficient Model Designs Paper Link Switch Transformers/Outrageously Large Neural Networks PDF, PDF MLA Attention PDF Mamba PDF Chapter III: Video Generation Subtopic 1: SOTA/Baseline Model Paper Link CogVideoX PDF HunyuanVideo PDF WAN PDF Seaweed-7B PDF Subtopic 2: Optimization Techniques Paper Link Pruning PDF Cache PDF Compression PDF Sparsity PDF Subtopic 3: Long Video Generation Paper Link Tuning-Free Multi-Event Long Video Generation PDF Long Context Tuning for Video Generation PDF One-Minute Video Generation with Test-Time Training PDF SKYREELS-V2 PDF Subtopic 4: Video Super Resolution Paper Link SeedVR PDF MGLD-VSR PDF DynamicScaler PDF Chapter IV: Secure LLM Subtopic 1: Diffusion Model/Flow Matching Paper Link DDIM PDF DDPM PDF Score-based PDF Flow Matching PDF Subtopic 2: Watermarking Paper Link HiDDeN PDF Stable Signature PDF WatermarkDM PDF AquaLoRA PDF Tree-Ring PDF Subtopic 3: System Optimization Techniques Paper Link ByteScheduler PDF PipeDream PDF APNN-TC PDF QGTC PDF Subtopic 4: Encryption Paper Link Reed-Solomon PDF Algebraic Soft-Decision Decoding of Reed–Solomon Codes PDF ZENO PDF Chapter V: MLLM Video Understanding Subtopic 1: SOTA/Baseline Paper Link Qwen2.5-VL PDF Storm PDF LLaVA PDF LLaMA PDF Seed1.5 VL PDF Kwai Keye-VL PDF Subtopic 2: System Optimization Techniques Paper Link ATP-LLaVA: Adaptive Token Pruning PDF FastVID: Dynamic Density Pruning PDF AdaReTaKe: Token Compression PDF Cocktail: Mixed-Precision Quantization PDF FastCache: KV-Cache Compression PDF Subtopic 3: Attention Kernel Optimization Paper Link AttentionEngine PDF SpargeAttn PDF FlexPrefill PDF MInference 1.0 PDF Subtopic 4: Algorithm Design Paper Link Adaptive Keyframe Sampling PDF Re-thinking Temporal Search PDF Improving LLM Video Understanding with 16 FPS PDF ","description":" Guest Lecture Materials Paper Link FlexAttention PDF FlexAttention(PyTorch Blog) Website Mercury PDF KPerfIR PDF ForestColl PDF Gemini PDF Chapter I: Kernel Related Subtopic 1: I/O Aware \u0026amp; Exact Attention Paper Link FlashAttention 1, 2, 3 PDF, PDF, PDF PagedAttention (vLLM) PDF SGLang PDF FlexAttention PDF FlashInfer PDF SpargeAttention PDF SageAttention 1,2 PDF, PDF Subtopic 2: Sparse Attention Paper Link DejaVu PDF H2O PDF SpAttn PDF MoE PDF Deepspeed-MoE PDF Subtopic 3: Kernel Generation \u0026amp; Compiler Paper Link TVM PDF Ansor PDF MLIR PDF Subtopic 4: Execution Optimization/Serving Paper Link Alpa PDF Orca PDF FlexGen PDF ZeRO-Offloading PDF Megatron-LM PDF FlashDecoding++ PDF SarathiServe PDF Chapter II: Efficient LLM Subtopic 1: LLM 101 Paper Link Attention is All You Need PDF BERT PDF GPT-3 PDF Scaling Laws PDF RLHF PDF PPO/DPO PDF , PDF Subtopic 2: Efficient Inference \u0026amp; Long-context Paper Link Streaming LLM \u0026amp; DuoAttention PDF, PDF MInference PDF H2O PDF TOVA/KIVI PDF, PDF Speculative Decoding PDF, PDF Multi-token prediction: Deepseek-v3 PDF Subtopic 3: Model Compression (Quant \u0026amp; Pruning) Paper Link LLM.int8()/GPTQ PDF, PDF AWQ PDF LLM Pruner PDF ShearedLlama PDF Subtopic 4: Efficient Training Paper Link ZeRO PDF Megatron-LM PDF LoRA \u0026amp; QLoRA PDF, PDF Subtopic 5: Efficient Model Designs Paper Link Switch Transformers/Outrageously Large Neural Networks PDF, PDF MLA Attention PDF Mamba PDF Chapter III: Video Generation Subtopic 1: SOTA/Baseline Model Paper Link CogVideoX PDF HunyuanVideo PDF WAN PDF Seaweed-7B PDF Subtopic 2: Optimization Techniques Paper Link Pruning PDF Cache PDF Compression PDF Sparsity PDF Subtopic 3: Long Video Generation Paper Link Tuning-Free Multi-Event Long Video Generation PDF Long Context Tuning for Video Generation PDF One-Minute Video Generation with Test-Time Training PDF SKYREELS-V2 PDF Subtopic 4: Video Super Resolution Paper Link SeedVR PDF MGLD-VSR PDF DynamicScaler PDF Chapter IV: Secure LLM Subtopic 1: Diffusion Model/Flow Matching Paper Link DDIM PDF DDPM PDF Score-based PDF Flow Matching PDF Subtopic 2: Watermarking Paper Link HiDDeN PDF Stable Signature PDF WatermarkDM PDF AquaLoRA PDF Tree-Ring PDF Subtopic 3: System Optimization Techniques Paper Link ByteScheduler PDF PipeDream PDF APNN-TC PDF QGTC PDF Subtopic 4: Encryption Paper Link Reed-Solomon PDF Algebraic Soft-Decision Decoding of Reed–Solomon Codes PDF ZENO PDF Chapter V: MLLM Video Understanding Subtopic 1: SOTA/Baseline Paper Link Qwen2.5-VL PDF Storm PDF LLaVA PDF LLaMA PDF Seed1.5 VL PDF Kwai Keye-VL PDF Subtopic 2: System Optimization Techniques Paper Link ATP-LLaVA: Adaptive Token Pruning PDF FastVID: Dynamic Density Pruning PDF AdaReTaKe: Token Compression PDF Cocktail: Mixed-Precision Quantization PDF FastCache: KV-Cache Compression PDF Subtopic 3: Attention Kernel Optimization Paper Link AttentionEngine PDF SpargeAttn PDF FlexPrefill PDF MInference 1.0 PDF Subtopic 4: Algorithm Design Paper Link Adaptive Keyframe Sampling PDF Re-thinking Temporal Search PDF Improving LLM Video Understanding with 16 FPS PDF "},{"id":21,"href":"/MLSys-Seminar/staffers/","title":"Staff","parent":"Machine Learning System Seminar","content":" ","description":""},{"id":22,"href":"/MLSys-Seminar/tags/","title":"Tags","parent":"Machine Learning System Seminar","content":"","description":""},{"id":23,"href":"/MLSys-Seminar/toc-tree/","title":"ToC-Tree","parent":"Machine Learning System Seminar","content":"This is just a demo section for the toc-tree shortcode.\nLevel 1 Level 1.1 Level 1.2 Level 1.3 Level 1.3.1 Level 2 Level 2.1 Level 2.2 ","description":"This is just a demo section for the toc-tree shortcode.\nLevel 1 Level 1.1 Level 1.2 Level 1.3 Level 1.3.1 Level 2 Level 2.1 Level 2.2 "}]