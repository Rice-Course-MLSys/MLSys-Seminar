---
title: "About"
sortReverse: true
---

This course focuses on efficient machine learning and systems. This is a crucial area as deep neural networks demand extraordinary levels of computation, hindering its deployment on everyday devices and burdening the cloud infrastructure. This course introduces efficient AI computing techniques that enable powerful deep learning applications on resource-constrained devices. 

---
**Topics include:**
- Model compression, pruning, quantization
- Neural architecture search
- Distributed training
- Data/model parallelism
- Gradient compression
- On-device fine-tuning
- Application-specific acceleration techniques for large language models and diffusion models

Students will get hands-on experience implementing model compression techniques and deploying large language models (Llama2-7B) on a laptop.

---

**Lecture Videos:** [https://live.efficientml.ai/](https://live.efficientml.ai/)  
**Time:** Tuesday/Thursday 3:35-5:00pm Eastern Time  
**Location:** 34-101  
**Office Hour:** Thursday 5:00-6:00 pm Eastern Time, 38-344 Meeting Room  
**Discussion:** Piazza  
**Homework Submission:** Canvas  

**Contact**:

For external inquiries, personal matters, or emergencies, you can email us at efficientml-staff [at] mit.edu. If you are interested in getting updates, please sign up here to join our mailing list to get notified!

**Prerequisites**:
6.191 Computation Structures and 6.390 Intro to Machine Learning. Students who don't full-fill the prerequisites will be de-registered in the second week of class. If you believe you have equivalent prior experience (e.g., a computer architecture course taken during your undergraduate studies at another institution), you may petition for consideration. Please submit your Petition Form by Sept. 6, 2024, 11:59:59 PM EST.


