---
title: Guest Lectures
---  

<style>
table th:first-of-type {
    width: 30%;
}
table th:nth-of-type(2) {
    width: 50%;
}
table th:nth-of-type(3) {
    width: 20%;
}

details summary {
    width: 200px;
    display: block; 
}
</style>

--- 
| Date   | Lecturer | Topic |
|--------|---------|-----------|
| <span style="float:left">Aug 29</span><span style="float:right">3:00 PM–3:40 PM</span> | **Yuke Wang** [[Slides]](./COMP620-MLSys-seminar_25Fall_lect1.pdf) | Introduction |
| <span style="float:left">Sep 5</span><span style="float:right">3:00 PM–3:40 PM</span> | **Boyuan Feng** [[Slides]](#)  <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>Boyuan Feng is a PyTorch Core Developer working on PyTorch Compiler, Inductor, CUDAGraph, and Flex Attention.</details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>FlexAttention is a novel compiler-driven programming model that allows implementing the majority of attention variants in a few lines of idiomatic PyTorch code. Since its release in PyTorch 2.5.0, many ML researchers have utilized it to customize their attention kernels without writing kernel code. In this talk, we present recent advances in FlexAttention. More details on our MLSys'25 paper (https://arxiv.org/pdf/2412.05496) and PyTorch Blog (https://pytorch.org/blog/flexattention-for-inference/)!</details></div> | FlexAttention |
| <span style="float:left">Sep 19</span><span style="float:right">3:00 PM–3:40 PM</span> | **Yue Guan** [[Slides]](#) <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>Yue Guan is a postdoctoral researcher at the University of California, San Diego, working with Prof. Yufei Ding in the Picasso Lab. He received his Ph.D. in Computer Science from Shanghai Jiao Tong University under the supervision of Prof. Jingwen Leng. His research focuses on efficient deep learning systems, spanning model compression, compiler optimization, and system design. His work has been published in top venues such as SOSP, OSDI, ASPLOS and HPCA.</details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>The rapid growth of large language models (LLMs) requires better compilers for efficient use of multi-GPU systems. In this talk, I will introduce Mercury, a compiler that manages remote GPU memory as part of the memory hierarchy to optimize computation, storage, and communication. I will also present KPerfIR, a tool that adds profiling directly into the compilation process to help analyze GPU kernel performance. These approaches show how integrating optimization and performance analysis in compilers can improve the scalability and efficiency of LLMs.</details></div> | Mercucy & KPerfIR |
| <span style="float:left">Oct 17</span><span style="float:right">3:00 PM–3:40 PM</span> | **Liangyu Zhao** [[Slides]](#) <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>Liangyu Zhao is a fourth-year PhD student at the University of Washington, advised by Prof. Arvind Krishnamurthy. His research focuses on machine learning systems, with an emphasis on network communication for distributed machine learning. Currently, he is a research scientist intern at Meta AI & Systems Co-Design team.</details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>As modern DNN models grow ever larger, collective communications between the accelerators (allreduce, etc.) emerge as a significant performance bottleneck. Designing efficient communication schedules is challenging, given today's heterogeneous and diverse network fabrics. We present ForestColl, a tool that generates throughput-optimal schedules for any network topology. ForestColl constructs broadcast/aggregation spanning trees as the communication schedule, achieving theoretical optimality. Its schedule generation runs in strongly polynomial time and is highly scalable. ForestColl supports any network fabrics, including both switching fabrics and direct accelerator connections. We evaluated ForestColl on multi-box AMD MI250 and NVIDIA DGX A100 platforms. ForestColl showed significant improvements over the vendors' own optimized communication libraries, RCCL and NCCL, across various settings and in LLM training. ForestColl also outperformed other state-of-the-art schedule generation techniques with both more efficient generated schedules and substantially faster schedule generation speed.</details></div> | ForestColl |
| <span style="float:left">Oct 31</span><span style="float:right">3:00 PM–3:40 PM</span> | **Zhuang Wang** [[Slides]](#) <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>Zhuang Wang is an Applied Scientist at Amazon Web Services AI. He received his Ph.D. degree in Computer Science from Rice University in 2023, fortunately advised by Prof. T. S. Eugene Ng. His current research interests focus on efficient training and inference systems for large language models.</details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>Frequent failures are observed during large model training due to large-scale resources involved and extended training time. This talk presents Gemini, a distributed training system that enables fast failure recovery for large model training by checkpointing to CPU memory of the host machines with much larger aggregated bandwidth. However, two challenges prevent naïvely checkpointing to CPU memory. First, the availability of checkpoints in CPU memory cannot be guaranteed when failures occur. Second, since the communication traffic for training and checkpointing share the same network, checkpoint traffic can interfere with training traffic and harm training throughput. To address these two challenges, we propose: 1) a provably near-optimal checkpoint placement strategy to maximize the probability of failure recovery from checkpoints in CPU memory; and 2) a checkpoint traffic scheduling algorithm to minimize, if not eliminate, the interference of checkpoint traffic on model training. Our evaluation shows that Gemini achieves optimal checkpoint frequency, i.e., every iteration, and incurs no overhead on training throughput for large model training.</details></div> | Gemini |
| <span style="float:left">Nov 14</span><span style="float:right">3:00 PM–3:40 PM</span> | **Yixin Dong** [[Slides]](#) <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>To Be Finished <img width=200/> </details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>Topic Not Confirmed</details></div> | [TBD] |