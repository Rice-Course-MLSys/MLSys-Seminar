---
title: Schedule
---  

<style>
table th:first-of-type {
    width: 30%;
}
table th:nth-of-type(2) {
    width: 50%;
}
table th:nth-of-type(3) {
    width: 10%;
}
table th:nth-of-type(4) {
    width: 10%;
}

details summary {
    width: 200px;
    display: block; 
}
</style>

--- 
| Date   | Lecture | Materials | Logistics | 
|--------|---------|-----------|-----------|
| <span style="float:left">Aug 29</span><span style="float:right">3:00 PM–4:15 PM</span> | Introduction [[Slides]](#)| | |
**Chapter I: Kernel Related**
| <span style="float:left">Sep 5</span><span style="float:right">3:00 PM–3:40 PM</span> | Guest Lecture: **Boyuan Feng** [[Slides]](#)  <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>Boyuan Feng is a PyTorch Core Developer working on PyTorch Compiler, Inductor, CUDAGraph, and Flex Attention.</details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>FlexAttention is a novel compiler-driven programming model that allows implementing the majority of attention variants in a few lines of idiomatic PyTorch code. Since its release in PyTorch 2.5.0, many ML researchers have utilized it to customize their attention kernels without writing kernel code. In this talk, we present recent advances in FlexAttention. More details on our MLSys'25 paper (https://arxiv.org/pdf/2412.05496) and PyTorch Blog (https://pytorch.org/blog/flexattention-for-inference/)!</details></div> | | |
| <span style="float:right">3:40 PM–4:15 PM</span> | I/O Aware & Exact Attention [[Slides]](#)| [[Link]](../materials/#subtopic-1-io-aware--exact-attention) | |
| <span style="float:left">Sep 12</span><span style="float:right">3:00 PM–4:15 PM</span> | Sparse Attention [[Slides]](#)| [[Link]](../materials/#subtopic-2-sparse-attention) | |
| <span style="float:left">Sep 19</span><span style="float:right">3:00 PM–3:40 PM</span> | Guest Lecture: **Yue Guan** [[Slides]](#) <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>Yue Guan is a postdoctoral researcher at the University of California, San Diego, working with Prof. Yufei Ding in the Picasso Lab. He received his Ph.D. in Computer Science from Shanghai Jiao Tong University under the supervision of Prof. Jingwen Leng. His research focuses on efficient deep learning systems, spanning model compression, compiler optimization, and system design. His work has been published in top venues such as SOSP, OSDI, ASPLOS and HPCA.</details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>The rapid growth of large language models (LLMs) requires better compilers for efficient use of multi-GPU systems. In this talk, I will introduce Mercury, a compiler that manages remote GPU memory as part of the memory hierarchy to optimize computation, storage, and communication. I will also present KPerfIR, a tool that adds profiling directly into the compilation process to help analyze GPU kernel performance. These approaches show how integrating optimization and performance analysis in compilers can improve the scalability and efficiency of LLMs.</details></div> | | |
| <span style="float:right">3:40 PM–4:15 PM</span> | Kernel Generation & Compiler [[Slides]](#)| [[Link]](../materials/#subtopic-3-kernel-generation--compiler) | |
| <span style="float:left">Sep 26</span><span style="float:right">3:00 PM–4:15 PM</span> | Execution Optimization/Serving [[Slides]](#)| [[Link]](../materials/#subtopic-4-execution-optimizationserving) | |
**Chapter II: Efficient LLM**
| <span style="float:left">Oct 3</span><span style="float:right">3:00 PM–3:40 PM</span> | Guest Lecture: **Liangyu Zhao** [[Slides]](#) <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>Liangyu Zhao is a fourth-year PhD student at the University of Washington, advised by Prof. Arvind Krishnamurthy. His research focuses on machine learning systems, with an emphasis on network communication for distributed machine learning. Currently, he is a research scientist intern at Meta AI & Systems Co-Design team.</details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>As modern DNN models grow ever larger, collective communications between the accelerators (allreduce, etc.) emerge as a significant performance bottleneck. Designing efficient communication schedules is challenging, given today's heterogeneous and diverse network fabrics. We present ForestColl, a tool that generates throughput-optimal schedules for any network topology. ForestColl constructs broadcast/aggregation spanning trees as the communication schedule, achieving theoretical optimality. Its schedule generation runs in strongly polynomial time and is highly scalable. ForestColl supports any network fabrics, including both switching fabrics and direct accelerator connections. We evaluated ForestColl on multi-box AMD MI250 and NVIDIA DGX A100 platforms. ForestColl showed significant improvements over the vendors' own optimized communication libraries, RCCL and NCCL, across various settings and in LLM training. ForestColl also outperformed other state-of-the-art schedule generation techniques with both more efficient generated schedules and substantially faster schedule generation speed.</details></div> | | |
| <span style="float:right">3:40 PM–4:15 PM</span> | LLM 101 [[Slides]](#)| [[Link]](../materials/#subtopic-1-llm-101) | |
| <span style="float:left">Oct 10</span><span style="float:right">3:00 PM–4:15 PM</span> | Efficient Inference & Long-context [[Slides]](#)| [[Link]](../materials/#subtopic-2-efficient-inference--long-context) | |
| <span style="float:left">Oct 17</span><span style="float:right">3:00 PM–3:40 PM</span> | Guest Lecture: **[Lecturer TBA]** [[Slides]](#) <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>To Be Determined <img width=200/> </details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>Topic Not Confirmed</details></div> | | |
| <span style="float:right">3:40 PM–4:15 PM</span> | Model Compression (Quant & Pruning) [[Slides]](#)| [[Link]](../materials/#subtopic-3-model-compression-quant--pruning) | |
| <span style="float:left">Oct 24</span><span style="float:right">3:00 PM–4:15 PM</span> | Efficient Training [[Slides]](#)| [[Link]](../materials/#subtopic-4-efficient-training) | |
| <span style="float:left">Oct 31</span><span style="float:right">3:00 PM–3:40 PM</span> | Guest Lecture: **Zhuang Wang** [[Slides]](#) <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>Zhuang Wang is an Applied Scientist at Amazon Web Services AI. He received his Ph.D. degree in Computer Science from Rice University in 2023, fortunately advised by Prof. T. S. Eugene Ng. His current research interests focus on efficient training and inference systems for large language models.</details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>Frequent failures are observed during large model training due to large-scale resources involved and extended training time. This talk presents Gemini, a distributed training system that enables fast failure recovery for large model training by checkpointing to CPU memory of the host machines with much larger aggregated bandwidth. However, two challenges prevent naïvely checkpointing to CPU memory. First, the availability of checkpoints in CPU memory cannot be guaranteed when failures occur. Second, since the communication traffic for training and checkpointing share the same network, checkpoint traffic can interfere with training traffic and harm training throughput. To address these two challenges, we propose: 1) a provably near-optimal checkpoint placement strategy to maximize the probability of failure recovery from checkpoints in CPU memory; and 2) a checkpoint traffic scheduling algorithm to minimize, if not eliminate, the interference of checkpoint traffic on model training. Our evaluation shows that Gemini achieves optimal checkpoint frequency, i.e., every iteration, and incurs no overhead on training throughput for large model training.</details></div> | | |
| <span style="float:right">3:40 PM–4:15 PM</span> | Efficient Model Designs [[Slides]](#)| [[Link]](../materials/#subtopic-5-efficient-model-designs) | |
**Chapter III: Video Generation**
| <span style="float:left">Nov 7</span><span style="float:right">3:00 PM–4:15 PM</span> | SOTA/Baseline Model [[Slides]](#)| [[Link]](../materials/#subtopic-1-sotabaseline-model) | |
| <span style="float:left">Nov 14</span><span style="float:right">3:00 PM–3:40 PM</span> | Guest Lecture: **Yixin Dong** [[Slides]](#) <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>To Be Finished <img width=200/> </details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>Topic Not Confirmed</details></div> | | |
| <span style="float:right">3:40 PM–4:15 PM</span> | Optimization Techniques [[Slides]](#)| [[Link]](../materials/#subtopic-2-optimization-techniques) | |
| <span style="float:left">Nov 21</span><span style="float:right">3:00 PM–4:15 PM</span> | Long Video Generation [[Slides]](#)| [[Link]](../materials/#subtopic-3-long-video-generation) | |
| <span style="float:left">Nov 28</span><span style="float:right">3:00 PM–3:40 PM</span> | Guest Lecture: **[Lecturer TBA]** [[Slides]](#) <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>To Be Determined <img width=200/> </details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>Topic Not Confirmed</details></div> | | |
| <span style="float:right">3:40 PM–4:15 PM</span> | Video Super Resolution [[Slides]](#)| [[Link]](../materials/#subtopic-4-video-super-resolution) | |
**Chapter IV: Secure LLM**
| <span style="float:left">Dec 5</span><span style="float:right">3:00 PM–4:15 PM</span> | Diffusion Model/Flow Matching [[Slides]](#)| [[Link]](../materials/#subtopic-1-diffusion-modelflow-matching) | |
| <span style="float:left">Dec 12</span><span style="float:right">3:00 PM–3:40 PM</span> | Guest Lecture: **[Lecturer TBA]** [[Slides]](#) <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>To Be Determined <img width=200/> </details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>Topic Not Confirmed</details></div> | | |
| <span style="float:right">3:40 PM–4:15 PM</span> | Watermarking [[Slides]](#)| [[Link]](../materials/#subtopic-2-watermarking) | |
| <span style="float:left">Dec 19</span><span style="float:right">3:00 PM–4:15 PM</span> | Efficient CNN [[Slides]](#)| [[Link]](../materials/#subtopic-3-efficient-cnn) | |
| <span style="float:left">Dec 26</span><span style="float:right">3:00 PM–3:40 PM</span> | Guest Lecture: **[Lecturer TBA]** [[Slides]](#) <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>To Be Determined <img width=200/> </details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>Topic Not Confirmed</details></div> | | |
| <span style="float:right">3:40 PM–4:15 PM</span> | Encryption [[Slides]](#)| [[Link]](../materials/#subtopic-4-encryption) | |
**Chapter V: MLLM Video Understanding**
| <span style="float:left">Jan 2</span><span style="float:right">3:00 PM–4:15 PM</span> | SOTA/Baseline [[Slides]](#)| [[Link]](../materials/#subtopic-1-sotabaseline) | |
| <span style="float:left">Jan 9</span><span style="float:right">3:00 PM–3:40 PM</span> | Guest Lecture: **[Lecturer TBA]** [[Slides]](#) <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>To Be Determined <img width=200/> </details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>Topic Not Confirmed</details></div> | | |
| <span style="float:right">3:40 PM–4:15 PM</span> | Optimization Techniques [[Slides]](#)| [[Link]](../materials/#subtopic-2-optimization-techniques-1) | |
| <span style="float:left">Jan 16</span><span style="float:right">3:00 PM–4:15 PM</span> | Algorithm Design [[Slides]](#)| [[Link]](../materials/#subtopic-3-algorithm-design) | |
| <span style="float:left">Jan 23</span><span style="float:right">3:00 PM–3:40 PM</span> | Guest Lecture: **[Lecturer TBA]** [[Slides]](#) <div style="display: inline-block;"><details><summary style="display: block; width: 100%; max-width: 200px">Bio</summary>To Be Determined <img width=200/> </details></div><div style="display: inline-block; margin-right: 10px;"><details><summary>Lecture Abstract</summary>Topic Not Confirmed</details></div> | | |
